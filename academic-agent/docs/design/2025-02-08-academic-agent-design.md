# å­¦æœ¯Agentç³»ç»Ÿè®¾è®¡æ–‡æ¡£

**æ—¥æœŸ**: 2025-02-08
**ç‰ˆæœ¬**: 2.0
**æ›´æ–°æ—¥æœŸ**: 2026-02-11
**çŠ¶æ€**: å®ç°é˜¶æ®µ

---

## å˜æ›´è®°å½•

### v2.0 (2026-02-11)
- **é‡å¤§æ¶æ„æ›´æ–°**: MCPæœåŠ¡å™¨å·²ä½œä¸ºæˆç†Ÿå­æ¨¡å—å­˜åœ¨
- **feedder-mcp** (v2.2.0): å®Œæ•´çš„RSS/Gmailè®ºæ–‡è·å–ã€è¿‡æ»¤ã€å¢å¼ºã€å¯¼å‡ºåŠŸèƒ½
- **logseq-mcp** (v2.1.1): 25ä¸ªMCPå·¥å…·ï¼Œ4å±‚æ¶æ„ï¼Œ158ä¸ªæµ‹è¯•
- **zotero-mcp** (v2.2.0): è¯­ä¹‰æœç´¢ã€PDFåˆ†æã€æ‰¹æ³¨æå–ã€æ‰¹é‡å·¥ä½œæµ
- **é‡æ–°èšç„¦**: ä¸»è¦å¼€å‘å·¥ä½œè½¬å‘Master Agentå’Œé›†æˆå±‚

### v1.0 (2025-02-08)
- åˆå§‹ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

---

## ç›®å½•

1. [ç³»ç»Ÿæ¶æ„æ¦‚è¿°](#1-ç³»ç»Ÿæ¶æ„æ¦‚è¿°)
2. [ç°æœ‰MCPæœåŠ¡å™¨](#2-ç°æœ‰mcpæœåŠ¡å™¨)
3. [Master Agentä¸å·¥å…·ç¼–æ’](#3-master-agentä¸å·¥å…·ç¼–æ’)
4. [æ•°æ®æµä¸æ ¸å¿ƒå·¥ä½œæµ](#4-æ•°æ®æµä¸æ ¸å¿ƒå·¥ä½œæµ)
5. [é›†æˆå±‚è®¾è®¡](#5-é›†æˆå±‚è®¾è®¡)
6. [é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ](#6-é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ)
7. [æµ‹è¯•ç­–ç•¥](#7-æµ‹è¯•ç­–ç•¥)
8. [éƒ¨ç½²ä¸é…ç½®](#8-éƒ¨ç½²ä¸é…ç½®)
9. [å¼€å‘è·¯çº¿å›¾](#9-å¼€å‘è·¯çº¿å›¾)
10. [å…³é”®æŠ€æœ¯ä¸å®ç°è¦ç‚¹](#10-å…³é”®æŠ€æœ¯ä¸å®ç°è¦ç‚¹)
11. [æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶](#11-æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶)

---

## 1. ç³»ç»Ÿæ¶æ„æ¦‚è¿°

### 1.1 æ ¸å¿ƒè®¾è®¡ç†å¿µ

è¿™æ˜¯ä¸€ä¸ªåŸºäºMCPï¼ˆModel Context Protocolï¼‰çš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿï¼Œé€šè¿‡ç»Ÿä¸€çš„Master Agentç¼–æ’ç°æœ‰çš„æˆç†ŸMCPæœåŠ¡å™¨ã€‚

**æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š**
- **å¤ç”¨ä¼˜å…ˆ**: ä¸‰ä¸ªæ ¸å¿ƒMCPæœåŠ¡å™¨å·²ä½œä¸ºå­æ¨¡å—å­˜åœ¨ï¼Œæ— éœ€ä»å¤´å¼€å‘
- **å•ä¸€å…¥å£**: ç”¨æˆ·åªä¸"å­¦æœ¯åŠ©æ‰‹"å¯¹è¯ï¼Œä¸éœ€è¦ç†è§£åº•å±‚MCPæ¶æ„
- **Zoteroä¸ºä¸­å¿ƒ**: Zoteroæ˜¯æ–‡çŒ®ç®¡ç†çš„å”¯ä¸€çœŸç›¸æºï¼ŒLogseqä½œä¸ºè§†å›¾å±‚
- **äº‘AIé©±åŠ¨**: æ‰€æœ‰AIåŠŸèƒ½ä½¿ç”¨å•†ä¸šAPIï¼ˆOpenAI/Anthropicï¼‰ç¡®ä¿æ•ˆæœ
- **ç”¨æˆ·æ§åˆ¶**: RSSæºå’ŒGmailæ ‡ç­¾å®Œå…¨ç”±ç”¨æˆ·é…ç½®ï¼ŒAgentä¸è‡ªåŠ¨æ¨æ–­

### 1.2 ç³»ç»Ÿåˆ†å±‚ï¼ˆæ›´æ–°ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    äº¤äº’å±‚                                â”‚
â”‚              Master Agent (ç”¨æˆ·å¯¹è¯)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç¼–æ’å±‚                                â”‚
â”‚          æ„å›¾è¯†åˆ« â†’ ä»»åŠ¡åˆ†è§£ â†’ å·¥å…·è°ƒç”¨                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æœåŠ¡å±‚ (å­æ¨¡å—)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚feedder   â”‚ â”‚  zotero  â”‚ â”‚  logseq  â”‚ â”‚  master  â”‚  â”‚
â”‚  â”‚  -mcp    â”‚ â”‚   -mcp   â”‚ â”‚   -mcp   â”‚ â”‚  _agent  â”‚  â”‚
â”‚  â”‚(v2.2.0)  â”‚ â”‚ (v2.2.0) â”‚ â”‚ (v2.1.1) â”‚ â”‚  (NEW)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â†“            â†“            â†“            â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚        é›†æˆå±‚ (academic-agentæ ¸å¿ƒ)                â”‚  â”‚
â”‚  â”‚  - Zotero-LogseqåŒæ­¥                             â”‚  â”‚
â”‚  â”‚  - å­¦æœ¯å·¥ä½œæµç¼–æ’                                 â”‚  â”‚
â”‚  â”‚  - æ¨¡æ¿ç®¡ç†                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Zotero  â”‚ â”‚  Logseq  â”‚ â”‚ChromaDB  â”‚                â”‚
â”‚  â”‚ (æ–‡çŒ®åº“) â”‚ â”‚ (ç¬”è®°)   â”‚ â”‚(å‘é‡ç´¢å¼•)â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 å­æ¨¡å—æ¶æ„

```
academic-agent/
â”œâ”€â”€ .gitmodules              # å­æ¨¡å—é…ç½®
â”œâ”€â”€ feedder-mcp/             # å­æ¨¡å—: è®ºæ–‡è·å–ä¸å¤„ç†
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ handlers/        # MCPå·¥å…·å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ services/        # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ sources/         # RSS/Gmailæ•°æ®æº
â”‚   â”‚   â”œâ”€â”€ filters/         # å…³é”®è¯/AIè¿‡æ»¤
â”‚   â”‚   â””â”€â”€ adapters/        # JSON/Zoteroå¯¼å‡º
â”‚   â””â”€â”€ tests/               # æµ‹è¯•å¥—ä»¶
â”œâ”€â”€ logseq-mcp/              # å­æ¨¡å—: Logseqé›†æˆ
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ handlers/        # 25ä¸ªMCPå·¥å…·
â”‚   â”‚   â”œâ”€â”€ services/        # ä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ models/          # æ•°æ®æ¨¡å‹
â”‚   â”‚   â””â”€â”€ client/          # Logseq APIå®¢æˆ·ç«¯
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ zotero-mcp/              # å­æ¨¡å—: Zoteroé›†æˆ
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ handlers/        # MCPå·¥å…·å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ services/        # æ ¸å¿ƒæœåŠ¡
â”‚   â”‚   â”œâ”€â”€ analyzer/        # PDFåˆ†æå™¨
â”‚   â”‚   â”œâ”€â”€ clients/         # Zotero/å¤–éƒ¨API
â”‚   â”‚   â””â”€â”€ models/          # æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ tests/
â””â”€â”€ src/                     # academic-agentæ ¸å¿ƒ
    â”œâ”€â”€ orchestrator/        # Master Agentç¼–æ’å±‚
    â”œâ”€â”€ workflows/           # å­¦æœ¯å·¥ä½œæµå®šä¹‰
    â”œâ”€â”€ integration/         # MCPé›†æˆé€‚é…å™¨
    â””â”€â”€ models/              # é€šç”¨æ•°æ®æ¨¡å‹
```

### 1.4 å…¸å‹å·¥ä½œæµç¤ºä¾‹

**ç”¨æˆ·è¯·æ±‚**: "å¸®æˆ‘æŠŠæœ€è¿‘ä¸€å‘¨arXivä¸Šå…³äºtransformerçš„è®ºæ–‡å¯¼å…¥æˆ‘çš„æ–‡çŒ®åº“"

```
1. Master Agentè§£ææ„å›¾
   â”œâ”€ è¯†åˆ«: æ–‡çŒ®å‘ç°ä»»åŠ¡
   â”œâ”€ æ—¶é—´èŒƒå›´: æœ€è¿‘ä¸€å‘¨
   â””â”€ å…³é”®è¯: transformer

2. è°ƒç”¨feedder_mcp.fetch_feeds()
   â”œâ”€ è·å–arXiv RSSæº
   â””â”€ è¿”å›åŸå§‹è®ºæ–‡åˆ—è¡¨

3. è°ƒç”¨feedder_mcp.filter()
   â”œâ”€ ä½¿ç”¨transformerå…³é”®è¯è¿‡æ»¤
   â””â”€ AIè¯­ä¹‰åŒ¹é…ï¼ˆå¦‚æœå¯ç”¨ï¼‰

4. è°ƒç”¨zotero_mcp.get_metadata()
   â”œâ”€ ä¸ç°æœ‰åº“å¯¹æ¯”å»é‡
   â””â”€ è¿”å›æ–°è®ºæ–‡åˆ—è¡¨

5. è°ƒç”¨feedder_mcp.enrich()
   â”œâ”€ CrossRefå…ƒæ•°æ®å¢å¼º
   â””â”€ OpenAlexå¼•ç”¨ä¿¡æ¯

6. è°ƒç”¨feedder_mcp.export()
   â”œâ”€ å¯¼å‡ºåˆ°Zotero
   â””â”€ è‡ªåŠ¨æ·»åŠ æ ‡ç­¾

7. è°ƒç”¨logseq_mcp.create_page()
   â”œâ”€ ä¸ºæ–°è®ºæ–‡åˆ›å»ºç¬”è®°é¡µé¢
   â”œâ”€ åº”ç”¨å­¦æœ¯æ¨¡æ¿
   â””â”€ å»ºç«‹Zoteroé“¾æ¥

8. è¿”å›ç»“æœç»™ç”¨æˆ·
   â””â”€ "å·²å¯¼å…¥15ç¯‡è®ºæ–‡ï¼Œå·²åˆ›å»ºLogseqç¬”è®°"
```

---

## 2. ç°æœ‰MCPæœåŠ¡å™¨

### 2.1 feedder-mcp (v2.2.0)

**ä»“åº“ä½ç½®**: `../feedder-mcp`

#### 2.1.1 åŠŸèƒ½æ¦‚è¿°

å®Œæ•´çš„è®ºæ–‡è·å–ã€è¿‡æ»¤ã€å¢å¼ºå’Œå¯¼å‡ºç³»ç»Ÿï¼Œæ”¯æŒRSSæºå’ŒGmailé›†æˆã€‚

**æ ¸å¿ƒåŠŸèƒ½:**
- MCPæœåŠ¡å™¨æ¨¡å¼ (stdio) + CLIå·¥å…·
- RSSè®¢é˜…æºç®¡ç† (OPMLæ”¯æŒ)
- Gmailè®ºæ–‡é‚®ä»¶è§£æ
- åŒé˜¶æ®µè¿‡æ»¤: å…³é”®è¯ + AIè¯­ä¹‰åŒ¹é…
- å…ƒæ•°æ®å¢å¼º (CrossRef + OpenAlex)
- å¯¼å‡ºé€‚é…å™¨ (JSON + Zotero)

#### 2.1.2 å¯ç”¨MCPå·¥å…·

```python
# è·å–å·¥å…·
@mcp.tool
def fetch_feeds(source: str = "rss", limit: int = 200, since: int = 15) -> str:
    """ä»RSSæºè·å–è®ºæ–‡"""

@mcp.tool
def filter_papers(
    input_path: str,
    keywords: List[str],
    use_ai: bool = True
) -> str:
    """è¿‡æ»¤è®ºæ–‡ï¼ˆå…³é”®è¯ + AIè¯­ä¹‰ï¼‰"""

@mcp.tool
def enrich_papers(
    input_path: str,
    api: str = "all",
    concurrency: int = 5
) -> str:
    """å¢å¼ºå…ƒæ•°æ®ï¼ˆCrossRef + OpenAlexï¼‰"""

@mcp.tool
def export_papers(
    input_path: str,
    output_path: str,
    format: str = "json"
) -> str:
    """å¯¼å‡ºè®ºæ–‡"""

@mcp.tool
def generate_keywords(topic: str) -> str:
    """AIç”Ÿæˆæœç´¢å…³é”®è¯"""
```

#### 2.1.3 CLIå·¥å…·

```bash
# 1. è·å–RSSè®ºæ–‡
feedder-mcp fetch --source rss --limit 200 --output output/raw.json

# 2. å…³é”®è¯è¿‡æ»¤
feedder-mcp filter --input output/raw.json --output output/filtered.json \
    --keywords battery zinc electrolyte

# 3. AIè¯­ä¹‰è¿‡æ»¤
feedder-mcp filter --input output/filtered.json --output output/ai_filtered.json \
    --keywords battery zinc

# 4. å…ƒæ•°æ®å¢å¼º
feedder-mcp enrich --input output/ai_filtered.json --output output/enriched.json \
    --api all --concurrency 5

# 5. å¯¼å‡ºåˆ°Zotero
feedder-mcp export --input output/enriched.json --format zotero
```

#### 2.1.4 é…ç½®ç¯å¢ƒå˜é‡

| å˜é‡ | è¯´æ˜ |
|------|------|
| `OPENAI_API_KEY` | AIè¿‡æ»¤å’Œå…³é”®è¯ç”Ÿæˆ |
| `RESEARCH_PROMPT` | ç”¨æˆ·ç ”ç©¶å…´è¶£æè¿° |
| `FEEDDER_MCP_OPML` | OPMLæ–‡ä»¶è·¯å¾„ |
| `GMAIL_TOKEN_JSON` | Gmail OAuthä»¤ç‰Œ |
| `ZOTERO_MCP_PATH` | Zotero MCPè·¯å¾„ï¼ˆç”¨äºå¯¼å‡ºï¼‰ |

---

### 2.2 logseq-mcp (v2.1.1)

**ä»“åº“ä½ç½®**: `../logseq-mcp`

#### 2.2.1 åŠŸèƒ½æ¦‚è¿°

å®Œæ•´çš„LogseqçŸ¥è¯†å›¾è°±é›†æˆï¼Œæä¾›25ä¸ªMCPå·¥å…·å’Œ6ä¸ªPromptsã€‚

**æ ¸å¿ƒç‰¹æ€§:**
- 25ä¸ªMCPå·¥å…·ï¼ˆå—æ“ä½œã€é¡µé¢æ“ä½œã€æŸ¥è¯¢ã€Gitæ“ä½œï¼‰
- 4å±‚æ¶æ„ï¼ˆåŸºç¡€è®¾æ–½ã€åŸŸã€åº”ç”¨ã€è¡¨ç°å±‚ï¼‰
- å®Œæ•´ç±»å‹å®‰å…¨ï¼ˆPydantic v2ï¼‰
- 158ä¸ªæµ‹è¯•ï¼Œ87%è¦†ç›–ç‡
- é‡è¯•é€»è¾‘å’ŒæŒ‡æ•°é€€é¿

#### 2.2.2 å¯ç”¨MCPå·¥å…·

**å—æ“ä½œï¼ˆ8ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def logseq_insert_block(
    content: str,
    parent_block: str = None,
    properties: dict = None
) -> str:
    """åˆ›å»ºæ–°å—"""

@mcp.tool
def logseq_update_block(uuid: str, content: str) -> str:
    """æ›´æ–°å—å†…å®¹"""

@mcp.tool
def logseq_delete_block(uuid: str) -> str:
    """åˆ é™¤å—"""

@mcp.tool
def logseq_get_block(uuid: str) -> str:
    """è·å–å—è¯¦æƒ…"""

@mcp.tool
def logseq_move_block(uuid: str, target_uuid: str) -> str:
    """ç§»åŠ¨å—"""

@mcp.tool
def logseq_insert_batch(parent: str, blocks: List[dict]) -> str:
    """æ‰¹é‡æ’å…¥å—"""

@mcp.tool
def logseq_get_page_blocks(page_name: str) -> str:
    """è·å–é¡µé¢æ‰€æœ‰å—"""

@mcp.tool
def logseq_get_current_page_content() -> str:
    """è·å–å½“å‰é¡µé¢å†…å®¹"""
```

**é¡µé¢æ“ä½œï¼ˆ5ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def logseq_create_page(
    page_name: str,
    properties: dict = None,
    journal: bool = False
) -> str:
    """åˆ›å»ºæ–°é¡µé¢"""

@mcp.tool
def logseq_get_page(page_name: str, include_children: bool = False) -> str:
    """è·å–é¡µé¢è¯¦æƒ…"""

@mcp.tool
def logseq_delete_page(page_name: str) -> str:
    """åˆ é™¤é¡µé¢"""

@mcp.tool
def logseq_rename_page(old_name: str, new_name: str) -> str:
    """é‡å‘½åé¡µé¢"""

@mcp.tool
def logseq_get_all_pages() -> str:
    """åˆ—å‡ºæ‰€æœ‰é¡µé¢"""
```

**æŸ¥è¯¢æ“ä½œï¼ˆ3ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def logseq_simple_query(query: str) -> str:
    """ç®€å•æŸ¥è¯¢"""

@mcp.tool
def logseq_advanced_query(query: str, inputs: List = None) -> str:
    """é«˜çº§DatascriptæŸ¥è¯¢"""

@mcp.tool
def logseq_get_tasks(marker: str = None, priority: str = None) -> str:
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
```

**ç¼–è¾‘å™¨æ“ä½œï¼ˆ5ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def logseq_get_current_page() -> str:
    """è·å–å½“å‰é¡µé¢"""

@mcp.tool
def logseq_get_current_block() -> str:
    """è·å–å½“å‰å—"""

@mcp.tool
def logseq_edit_block(uuid: str, pos: int = 0) -> str:
    """ç¼–è¾‘å—"""

@mcp.tool
def logseq_exit_editing_mode(select_block: bool = False) -> str:
    """é€€å‡ºç¼–è¾‘æ¨¡å¼"""

@mcp.tool
def logseq_get_editing_content() -> str:
    """è·å–ç¼–è¾‘å†…å®¹"""
```

**å›¾æ“ä½œï¼ˆ2ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def logseq_get_current_graph() -> str:
    """è·å–å½“å‰å›¾ä¿¡æ¯"""

@mcp.tool
def logseq_get_user_configs() -> str:
    """è·å–ç”¨æˆ·é…ç½®"""
```

**Gitæ“ä½œï¼ˆ2ä¸ªå·¥å…·ï¼Œéœ€å¯ç”¨ï¼‰:**
```python
@mcp.tool
def logseq_git_commit(message: str) -> str:
    """Gitæäº¤"""

@mcp.tool
def logseq_git_status() -> str:
    """GitçŠ¶æ€"""
```

#### 2.2.3 é…ç½®ç¯å¢ƒå˜é‡

| å˜é‡ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `LOGSEQ_API_TOKEN` | APIæˆæƒä»¤ç‰Œ | å¿…éœ€ |
| `LOGSEQ_API_URL` | APIç«¯ç‚¹ | `http://localhost:12315` |
| `LOGSEQ_ENABLE_ADVANCED_QUERIES` | å¯ç”¨é«˜çº§æŸ¥è¯¢ | `true` |
| `LOGSEQ_ENABLE_GIT_OPERATIONS` | å¯ç”¨Gitæ“ä½œ | `false` |
| `LOGSEQ_API_TIMEOUT` | APIè¶…æ—¶ï¼ˆç§’ï¼‰ | `10` |
| `LOGSEQ_API_MAX_RETRIES` | æœ€å¤§é‡è¯•æ¬¡æ•° | `3` |

---

### 2.3 zotero-mcp (v2.2.0)

**ä»“åº“ä½ç½®**: `../zotero-mcp`

#### 2.3.1 åŠŸèƒ½æ¦‚è¿°

åŠŸèƒ½å…¨é¢çš„Zoteroé›†æˆï¼Œæ”¯æŒè¯­ä¹‰æœç´¢ã€PDFåˆ†æã€æ‰¹æ³¨æå–ã€æ‰¹é‡å·¥ä½œæµã€‚

**æ ¸å¿ƒç‰¹æ€§:**
- å¤šæ¨¡æ€PDFåˆ†æï¼ˆOCR + å›¾åƒæå–ï¼‰
- è¯­ä¹‰æœç´¢ï¼ˆå‘é‡æ•°æ®åº“ï¼‰
- å…ƒæ•°æ®å¢å¼ºï¼ˆCrossRef/OpenAlexï¼‰
- æ‰¹æ³¨å’Œç¬”è®°æå–
- é‡å¤æ£€æµ‹å’Œåˆ é™¤
- æ£€æŸ¥ç‚¹/æ¢å¤æœºåˆ¶
- å¯é…ç½®è¾“å‡ºæ¨¡æ¿

#### 2.3.2 å¯ç”¨MCPå·¥å…·

**æœç´¢ä¸å‘ç°ï¼ˆ5ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def zotero_semantic_search(query: str, limit: int = 10) -> str:
    """AIè¯­ä¹‰æœç´¢"""

@mcp.tool
def zotero_search(query: str) -> str:
    """å…³é”®è¯æœç´¢"""

@mcp.tool
def zotero_advanced_search(criteria: dict) -> str:
    """é«˜çº§æœç´¢"""

@mcp.tool
def zotero_search_by_tag(tag: str) -> str:
    """æ ‡ç­¾æœç´¢"""

@mcp.tool
def zotero_get_recent(limit: int = 20) -> str:
    """æœ€è¿‘æ·»åŠ """
```

**å†…å®¹è®¿é—®ï¼ˆ4ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def zotero_get_metadata(item_key: str) -> str:
    """è·å–å…ƒæ•°æ®"""

@mcp.tool
def zotero_get_fulltext(item_key: str) -> str:
    """è·å–å…¨æ–‡"""

@mcp.tool
def zotero_get_bundle(item_key: str) -> str:
    """è·å–å®Œæ•´æ•°æ®åŒ…"""

@mcp.tool
def zotero_get_children(item_key: str) -> str:
    """è·å–é™„ä»¶å’Œç¬”è®°"""
```

**æ‰¹æ³¨å’Œç¬”è®°ï¼ˆ4ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def zotero_get_annotations(item_key: str) -> str:
    """è·å–PDFæ‰¹æ³¨"""

@mcp.tool
def zotero_get_notes(item_key: str = None) -> str:
    """è·å–ç¬”è®°"""

@mcp.tool
def zotero_search_notes(query: str) -> str:
    """æœç´¢ç¬”è®°/æ‰¹æ³¨"""

@mcp.tool
def zotero_create_note(item_key: str, content: str) -> str:
    """åˆ›å»ºç¬”è®°"""
```

**æ‰¹é‡å·¥ä½œæµï¼ˆ4ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def zotero_prepare_analysis(
    item_keys: List[str],
    analysis_type: str = "comprehensive"
) -> str:
    """å‡†å¤‡PDFåˆ†æ"""

@mcp.tool
def zotero_batch_analyze_pdfs(
    workflow_id: str,
    model: str = "deepseek-chat"
) -> str:
    """æ‰¹é‡åˆ†æPDF"""

@mcp.tool
def zotero_resume_workflow(workflow_id: str) -> str:
    """æ¢å¤ä¸­æ–­çš„å·¥ä½œæµ"""

@mcp.tool
def zotero_list_workflows() -> str:
    """åˆ—å‡ºå·¥ä½œæµçŠ¶æ€"""
```

**PDFå‘ç°ï¼ˆ2ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def zotero_find_pdf_si(item_key: str) -> str:
    """æŸ¥æ‰¾PDF/SIï¼ˆå•ä¸ªï¼‰"""

@mcp.tool
def zotero_find_pdf_si_batch(collection_name: str) -> str:
    """æ‰¹é‡æŸ¥æ‰¾PDF/SI"""
```

**é›†åˆå’Œæ ‡ç­¾ï¼ˆ3ä¸ªå·¥å…·ï¼‰:**
```python
@mcp.tool
def zotero_get_collections() -> str:
    """åˆ—å‡ºé›†åˆ"""

@mcp.tool
def zotero_find_collection(name: str) -> str:
    """æŸ¥æ‰¾é›†åˆ"""

@mcp.tool
def zotero_get_tags() -> str:
    """åˆ—å‡ºæ‰€æœ‰æ ‡ç­¾"""
```

#### 2.3.3 CLIå·¥å…·

```bash
# è¯­ä¹‰æœç´¢æ•°æ®åº“ç®¡ç†
zotero-mcp update-db                      # æ›´æ–°æ•°æ®åº“
zotero-mcp update-db --fulltext           # å…¨æ–‡æ›´æ–°
zotero-mcp db-status                      # æ£€æŸ¥çŠ¶æ€

# ç ”ç©¶å·¥ä½œæµ
zotero-mcp scan                            # æ‰«ææœªå¤„ç†è®ºæ–‡
zotero-mcp update-metadata                 # å¢å¼ºå…ƒæ•°æ®
zotero-mcp deduplicate                     # å»é‡
zotero-mcp pdf-find --item-key ABCD1234   # æŸ¥æ‰¾PDF

# æ›´æ–°å’Œç»´æŠ¤
zotero-mcp update                          # æ›´æ–°ç‰ˆæœ¬
zotero-mcp version                         # æ˜¾ç¤ºç‰ˆæœ¬
```

#### 2.3.4 é…ç½®ç¯å¢ƒå˜é‡

**Zoteroè¿æ¥:**
```bash
ZOTERO_LOCAL=true                    # æœ¬åœ°APIï¼ˆé»˜è®¤ï¼‰
ZOTERO_API_KEY=your_key             # Web APIå¯†é’¥
ZOTERO_LIBRARY_ID=your_id           # Web APIåº“ID
```

**è¯­ä¹‰æœç´¢:**
```bash
ZOTERO_EMBEDDING_MODEL=default      # default, openai, gemini
OPENAI_API_KEY=your_key
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

**æ‰¹é‡åˆ†æ:**
```bash
DEEPSEEK_API_KEY=your_key
DEEPSEEK_MODEL=deepseek-chat
```

**å¤šæ¨¡æ€åˆ†æ:**
```bash
ZOTERO_MCP_CLI_LLM_PROVIDER=deepseek
ZOTERO_MCP_CLI_LLM_MODEL=deepseek-chat
ZOTERO_MCP_CLI_LLM_OCR_ENABLED=true
ZOTERO_MCP_CLI_LLM_MAX_PAGES=50
```

---

## 3. Master Agentä¸å·¥å…·ç¼–æ’

### 3.1 Master Agentçš„èŒè´£

Master Agentæ˜¯ç³»ç»Ÿçš„åè°ƒè€…ï¼Œè´Ÿè´£ï¼š
1. **ç†è§£ç”¨æˆ·æ„å›¾**: å°†è‡ªç„¶è¯­è¨€è¯·æ±‚è½¬æ¢ä¸ºå·¥ä½œæµ
2. **ç¼–æ’MCPå·¥å…·**: è°ƒç”¨å­æ¨¡å—ä¸­çš„å·¥å…·å®Œæˆå¤æ‚ä»»åŠ¡
3. **ç®¡ç†ä¸Šä¸‹æ–‡**: ç»´æŠ¤å¯¹è¯çŠ¶æ€å’Œç”¨æˆ·ç ”ç©¶å…´è¶£
4. **å¤„ç†é”™è¯¯**: ä¼˜é›…é™çº§å’Œé”™è¯¯æ¢å¤
5. **æä¾›å­¦æœ¯å·¥ä½œæµ**: é¢„å®šä¹‰çš„å¸¸è§ç ”ç©¶ä»»åŠ¡æµç¨‹

### 3.2 æ ¸å¿ƒèƒ½åŠ›

#### 3.2.1 æ„å›¾è¯†åˆ«ä¸ä»»åŠ¡åˆ†è§£

```python
from enum import Enum
from typing import List, Tuple
from dataclasses import dataclass

class Intent(Enum):
    PAPER_DISCOVERY = "paper_discovery"        # è®ºæ–‡å‘ç°
    PAPER_ANALYSIS = "paper_analysis"          # è®ºæ–‡åˆ†æ
    LITERATURE_REVIEW = "literature_review"    # æ–‡çŒ®ç»¼è¿°
    KNOWLEDGE_QUERY = "knowledge_query"        # çŸ¥è¯†æŸ¥è¯¢
    WORKFLOW_AUTOMATION = "workflow_automation" # å·¥ä½œæµè‡ªåŠ¨åŒ–

@dataclass
class WorkflowStep:
    mcp_server: str      # "feedder", "zotero", "logseq"
    tool_name: str       # å·¥å…·åç§°
    parameters: dict     # å·¥å…·å‚æ•°
    depends_on: List[str] = None  # ä¾èµ–çš„æ­¥éª¤ID

@dataclass
class Workflow:
    intent: Intent
    steps: List[WorkflowStep]
    description: str

def decompose_user_query(query: str) -> Workflow:
    """å°†ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºMCPå·¥å…·è°ƒç”¨åºåˆ—"""

    # ç¤ºä¾‹1: æ–‡çŒ®å‘ç°
    if any(kw in query.lower() for kw in ["æ‰¾è®ºæ–‡", "æ–°è®ºæ–‡", "å¯¼å…¥", "å‘ç°"]):
        return Workflow(
            intent=Intent.PAPER_DISCOVERY,
            description="ä»RSSæºå‘ç°å¹¶å¯¼å…¥ç›¸å…³è®ºæ–‡",
            steps=[
                WorkflowStep(
                    mcp_server="feedder",
                    tool_name="fetch_feeds",
                    parameters={"limit": 200, "since": 7}
                ),
                WorkflowStep(
                    mcp_server="feedder",
                    tool_name="generate_keywords",
                    parameters={"topic": extract_topic(query)}
                ),
                WorkflowStep(
                    mcp_server="feedder",
                    tool_name="filter_papers",
                    parameters={"use_ai": True}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="get_metadata",
                    parameters={"limit": 1000}
                ),
                WorkflowStep(
                    mcp_server="feedder",
                    tool_name="enrich_papers",
                    parameters={"api": "all"}
                ),
                WorkflowStep(
                    mcp_server="feedder",
                    tool_name="export_papers",
                    parameters={"format": "zotero"}
                ),
                WorkflowStep(
                    mcp_server="logseq",
                    tool_name="create_paper_pages",
                    parameters={}
                )
            ]
        )

    # ç¤ºä¾‹2: æ–‡çŒ®åˆ†æ
    elif any(kw in query.lower() for kw in ["åˆ†æ", "ç†è§£", "æ€»ç»“"]):
        return Workflow(
            intent=Intent.PAPER_ANALYSIS,
            description="æ·±åº¦åˆ†æè®ºæ–‡å†…å®¹",
            steps=[
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="get_fulltext",
                    parameters={"item_key": extract_item_key(query)}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="prepare_analysis",
                    parameters={"analysis_type": "comprehensive"}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="batch_analyze_pdfs",
                    parameters={"model": "deepseek-chat"}
                ),
                WorkflowStep(
                    mcp_server="logseq",
                    tool_name="update_page_block",
                    parameters={"section": "AIåˆ†æ"}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="semantic_search",
                    parameters={"limit": 5}
                ),
                WorkflowStep(
                    mcp_server="logseq",
                    tool_name="add_related_papers",
                    parameters={}
                )
            ]
        )

    # ç¤ºä¾‹3: æ–‡çŒ®ç»¼è¿°
    elif any(kw in query.lower() for kw in ["ç»¼è¿°", "å›é¡¾", "æ€»ç»“ç ”ç©¶"]):
        return Workflow(
            intent=Intent.LITERATURE_REVIEW,
            description="ç”Ÿæˆä¸»é¢˜æ–‡çŒ®ç»¼è¿°",
            steps=[
                WorkflowStep(
                    mcp_server="feedder",
                    tool_name="generate_keywords",
                    parameters={"topic": extract_topic(query)}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="semantic_search",
                    parameters={"limit": 20}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="get_fulltext_batch",
                    parameters={}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="batch_analyze_pdfs",
                    parameters={}
                ),
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="synthesize_review",
                    parameters={}
                ),
                WorkflowStep(
                    mcp_server="logseq",
                    tool_name="create_review_page",
                    parameters={}
                )
            ]
        )

    # ç¤ºä¾‹4: çŸ¥è¯†æŸ¥è¯¢
    else:
        return Workflow(
            intent=Intent.KNOWLEDGE_QUERY,
            description="æŸ¥è¯¢æ–‡çŒ®åº“å’Œç¬”è®°",
            steps=[
                WorkflowStep(
                    mcp_server="zotero",
                    tool_name="semantic_search",
                    parameters={"query": query, "limit": 10}
                ),
                WorkflowStep(
                    mcp_server="logseq",
                    tool_name="simple_query",
                    parameters={"query": query}
                )
            ]
        )
```

#### 3.2.2 ä¸Šä¸‹æ–‡ç®¡ç†

```python
from typing import Dict, Any, Optional
from datetime import datetime
import json

class ConversationContext:
    """ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡"""

    def __init__(self):
        self.user_profile = {
            "research_interests": [],
            "recent_queries": [],
            "zotero_library_snapshot": None,
            "preferences": {
                "auto_sync_logseq": True,
                "default_analysis_type": "comprehensive",
                "max_papers_to_import": 50
            }
        }
        self.active_items = {}  # å½“å‰è®¨è®ºçš„æ–‡çŒ®
        self.workflow_history = []
        self.last_update = datetime.now()

    def remember(self, key: str, value: Any):
        """è®°ä½ä¿¡æ¯ä¾›åç»­ä½¿ç”¨"""
        self.user_profile[key] = value
        self.last_update = datetime.now()

    def recall(self, key: str, default: Any = None) -> Any:
        """å¬å›ä¹‹å‰çš„ä¿¡æ¯"""
        return self.user_profile.get(key, default)

    def add_active_item(self, item_key: str, metadata: dict):
        """æ·»åŠ å½“å‰è®¨è®ºçš„æ–‡çŒ®"""
        self.active_items[item_key] = {
            "metadata": metadata,
            "added_at": datetime.now()
        }

    def get_active_item(self, item_key: str = None) -> Optional[dict]:
        """è·å–å½“å‰æˆ–æœ€æ–°çš„æ´»è·ƒæ–‡çŒ®"""
        if item_key:
            return self.active_items.get(item_key)
        if self.active_items:
            # è¿”å›æœ€è¿‘æ·»åŠ çš„
            return max(self.active_items.items(),
                      key=lambda x: x[1]["added_at"])[1]
        return None

    def add_workflow(self, workflow: Workflow):
        """è®°å½•å·¥ä½œæµå†å²"""
        self.workflow_history.append({
            "workflow": workflow,
            "timestamp": datetime.now()
        })

    def to_dict(self) -> dict:
        """åºåˆ—åŒ–ä¸ºå­—å…¸"""
        return {
            "user_profile": self.user_profile,
            "active_items": {
                k: {
                    "metadata": v["metadata"],
                    "added_at": v["added_at"].isoformat()
                }
                for k, v in self.active_items.items()
            },
            "workflow_history": [
                {
                    "intent": w["workflow"].intent.value,
                    "description": w["workflow"].description,
                    "timestamp": w["timestamp"].isoformat()
                }
                for w in self.workflow_history[-10:]  # æœ€è¿‘10æ¡
            ],
            "last_update": self.last_update.isoformat()
        }

    def save(self, path: str):
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)

    @classmethod
    def load(cls, path: str) -> 'ConversationContext':
        """ä»æ–‡ä»¶åŠ è½½"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        ctx = cls()
        ctx.user_profile = data.get("user_profile", {})
        # æ¢å¤å…¶ä»–å­—æ®µ...
        return ctx

# ä½¿ç”¨ç¤ºä¾‹
ctx = ConversationContext()

# ç”¨æˆ·æåˆ°ç ”ç©¶å…´è¶£
ctx.remember("research_interests", ["transformer", "attention", "NLP"])

# å¯¼å…¥è®ºæ–‡åè®°å½•
ctx.add_active_item("ABCD1234", {
    "title": "Attention Is All You Need",
    "authors": ["Vaswani, A."],
    "year": 2017
})

# ç”¨æˆ·é—®: "åˆšæ‰é‚£ç¯‡è®ºæ–‡çš„ä½œè€…è¿˜æœ‰ä»€ä¹ˆå…¶ä»–å·¥ä½œï¼Ÿ"
paper = ctx.get_active_item()
if paper:
    authors = paper["metadata"]["authors"]
    # ä½¿ç”¨ä½œè€…åè¿›è¡Œæœç´¢...
```

#### 3.2.3 å·¥å…·é€‰æ‹©ç­–ç•¥

```python
from typing import List, Dict, Callable

# MCPå·¥å…·æ˜ å°„
MCP_TOOLS = {
    "feedder": {
        "fetch_feeds": {
            "description": "ä»RSSæºè·å–è®ºæ–‡",
            "parameters": {"source": "rss", "limit": 200, "since": 15}
        },
        "filter_papers": {
            "description": "è¿‡æ»¤è®ºæ–‡ï¼ˆå…³é”®è¯+AIï¼‰",
            "parameters": {"input_path": "", "keywords": [], "use_ai": True}
        },
        "enrich_papers": {
            "description": "å¢å¼ºå…ƒæ•°æ®",
            "parameters": {"input_path": "", "api": "all"}
        },
        "export_papers": {
            "description": "å¯¼å‡ºè®ºæ–‡",
            "parameters": {"input_path": "", "format": "json"}
        },
        "generate_keywords": {
            "description": "AIç”Ÿæˆå…³é”®è¯",
            "parameters": {"topic": ""}
        }
    },
    "zotero": {
        "semantic_search": {
            "description": "è¯­ä¹‰æœç´¢",
            "parameters": {"query": "", "limit": 10}
        },
        "search": {
            "description": "å…³é”®è¯æœç´¢",
            "parameters": {"query": ""}
        },
        "get_metadata": {
            "description": "è·å–å…ƒæ•°æ®",
            "parameters": {"item_key": ""}
        },
        "get_fulltext": {
            "description": "è·å–å…¨æ–‡",
            "parameters": {"item_key": ""}
        },
        "get_annotations": {
            "description": "è·å–æ‰¹æ³¨",
            "parameters": {"item_key": ""}
        },
        "prepare_analysis": {
            "description": "å‡†å¤‡PDFåˆ†æ",
            "parameters": {"item_keys": [], "analysis_type": "comprehensive"}
        },
        "batch_analyze_pdfs": {
            "description": "æ‰¹é‡åˆ†æPDF",
            "parameters": {"workflow_id": "", "model": "deepseek-chat"}
        },
        "get_recent": {
            "description": "æœ€è¿‘æ·»åŠ ",
            "parameters": {"limit": 20}
        }
    },
    "logseq": {
        "create_page": {
            "description": "åˆ›å»ºé¡µé¢",
            "parameters": {"page_name": "", "properties": {}}
        },
        "get_page": {
            "description": "è·å–é¡µé¢",
            "parameters": {"page_name": "", "include_children": True}
        },
        "insert_block": {
            "description": "æ’å…¥å—",
            "parameters": {"content": "", "parent_block": ""}
        },
        "update_block": {
            "description": "æ›´æ–°å—",
            "parameters": {"uuid": "", "content": ""}
        },
        "simple_query": {
            "description": "ç®€å•æŸ¥è¯¢",
            "parameters": {"query": ""}
        },
        "get_all_pages": {
            "description": "åˆ—å‡ºæ‰€æœ‰é¡µé¢",
            "parameters": {}
        }
    }
}

# æ„å›¾åˆ°å·¥å…·çš„æ˜ å°„
INTENT_TOOL_MAPPING = {
    Intent.PAPER_DISCOVERY: [
        "feedder.fetch_feeds",
        "feedder.generate_keywords",
        "feedder.filter_papers",
        "zotero.get_metadata",
        "feedder.enrich_papers",
        "feedder.export_papers",
        "logseq.create_page"
    ],
    Intent.PAPER_ANALYSIS: [
        "zotero.get_fulltext",
        "zotero.prepare_analysis",
        "zotero.batch_analyze_pdfs",
        "logseq.insert_block",
        "zotero.semantic_search"
    ],
    Intent.LITERATURE_REVIEW: [
        "feedder.generate_keywords",
        "zotero.semantic_search",
        "zotero.get_fulltext",
        "zotero.batch_analyze_pdfs",
        "logseq.create_page"
    ],
    Intent.KNOWLEDGE_QUERY: [
        "zotero.semantic_search",
        "logseq.simple_query"
    ]
}

class ToolSelector:
    """å·¥å…·é€‰æ‹©å™¨"""

    def __init__(self, context: ConversationContext):
        self.context = context

    def select_tools_for_intent(self, intent: Intent) -> List[str]:
        """æ ¹æ®æ„å›¾é€‰æ‹©å·¥å…·"""
        return INTENT_TOOL_MAPPING.get(intent, [])

    def get_tool_definition(self, tool_path: str) -> Dict:
        """è·å–å·¥å…·å®šä¹‰"""
        mcp, tool = tool_path.split('.')
        return MCP_TOOLS[mcp][tool]

    def adapt_parameters(self, tool_path: str, base_params: dict) -> dict:
        """æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´å‚æ•°"""
        definition = self.get_tool_definition(tool_path)
        params = definition["parameters"].copy()
        params.update(base_params)

        # æ ¹æ®ç”¨æˆ·åå¥½è°ƒæ•´
        if tool_path == "feedder.filter_papers":
            params["use_ai"] = self.context.recall(
                "use_ai_filtering",
                default=True
            )

        return params
```

#### 3.2.4 æ‰§è¡Œå¼•æ“

```python
import asyncio
from typing import Any, Dict
import subprocess
import json

class MCPExecutor:
    """MCPå·¥å…·æ‰§è¡Œå™¨"""

    def __init__(self, context: ConversationContext):
        self.context = context
        self.selector = ToolSelector(context)

    async def execute_tool(self, mcp: str, tool: str, params: dict) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªMCPå·¥å…·"""

        # æ ¹æ®MCPæœåŠ¡å™¨ç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹å¼
        if mcp == "feedder":
            return await self._execute_feedder(tool, params)
        elif mcp == "zotero":
            return await self._execute_zotero(tool, params)
        elif mcp == "logseq":
            return await self._execute_logseq(tool, params)
        else:
            raise ValueError(f"Unknown MCP server: {mcp}")

    async def _execute_feedder(self, tool: str, params: dict) -> Dict:
        """æ‰§è¡Œfeedder-mcpå·¥å…·"""

        # ä½¿ç”¨CLIæˆ–MCPåè®®
        if tool == "fetch_feeds":
            cmd = [
                "feedder-mcp", "fetch",
                "--limit", str(params.get("limit", 200)),
                "--since", str(params.get("since", 15)),
                "--output", "output/raw.json"
            ]
        elif tool == "filter_papers":
            cmd = [
                "feedder-mcp", "filter",
                "--input", params["input_path"],
                "--output", "output/filtered.json"
            ]
            if params.get("keywords"):
                cmd.extend(["--keywords"] + params["keywords"])
            if params.get("use_ai"):
                cmd.append("--ai")
        elif tool == "generate_keywords":
            # éœ€è¦AI APIè°ƒç”¨
            return await self._generate_keywords_ai(params["topic"])
        else:
            raise ValueError(f"Unknown feedder tool: {tool}")

        result = subprocess.run(cmd, capture_output=True, text=True)
        return {"status": "success" if result.returncode == 0 else "error",
                "output": result.stdout,
                "error": result.stderr}

    async def _execute_zotero(self, tool: str, params: dict) -> Dict:
        """æ‰§è¡Œzotero-mcpå·¥å…·"""

        if tool == "semantic_search":
            cmd = [
                "zotero-mcp", "query", params["query"],
                "--limit", str(params.get("limit", 10))
            ]
        elif tool == "get_fulltext":
            cmd = [
                "zotero-mcp", "export", params["item_key"],
                "--fulltext"
            ]
        elif tool == "batch_analyze_pdfs":
            cmd = [
                "zotero-mcp", "analyze",
                "--workflow-id", params["workflow_id"],
                "--model", params.get("model", "deepseek-chat")
            ]
        else:
            raise ValueError(f"Unknown zotero tool: {tool}")

        result = subprocess.run(cmd, capture_output=True, text=True)
        return {"status": "success" if result.returncode == 0 else "error",
                "output": result.stdout,
                "error": result.stderr}

    async def _execute_logseq(self, tool: str, params: dict) -> Dict:
        """æ‰§è¡Œlogseq-mcpå·¥å…·"""

        if tool == "create_page":
            cmd = [
                "logseq-mcp", "pages", "create",
                "--name", params["page_name"]
            ]
            if params.get("properties"):
                cmd.extend(["--properties", json.dumps(params["properties"])])
        elif tool == "insert_block":
            cmd = [
                "logseq-mcp", "blocks", "insert",
                "--parent", params.get("parent_block", ""),
                "--content", params["content"]
            ]
        elif tool == "simple_query":
            cmd = [
                "logseq-mcp", "queries", "simple",
                "--query", params["query"]
            ]
        else:
            raise ValueError(f"Unknown logseq tool: {tool}")

        result = subprocess.run(cmd, capture_output=True, text=True)
        return {"status": "success" if result.returncode == 0 else "error",
                "output": result.stdout,
                "error": result.stderr}

    async def _generate_keywords_ai(self, topic: str) -> Dict:
        """ä½¿ç”¨AIç”Ÿæˆå…³é”®è¯"""
        # è¿™é‡Œå¯ä»¥è°ƒç”¨OpenAI APIæˆ–ä½¿ç”¨feedder-mcpçš„å†…ç½®åŠŸèƒ½
        # ç®€åŒ–ç¤ºä¾‹ï¼š
        return {
            "status": "success",
            "keywords": {
                "primary": [topic],
                "secondary": [],
                "related": []
            }
        }

    async def execute_workflow(self, workflow: Workflow) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´å·¥ä½œæµ"""

        results = {
            "workflow": workflow.description,
            "steps_completed": [],
            "steps_failed": [],
            "final_result": None
        }

        for step in workflow.steps:
            try:
                # æ£€æŸ¥ä¾èµ–
                if step.depends_on:
                    if not all(d in results["steps_completed"] for d in step.depends_on):
                        continue

                # æ‰§è¡Œå·¥å…·
                result = await self.execute_tool(
                    step.mcp_server,
                    step.tool_name,
                    step.parameters
                )

                if result["status"] == "success":
                    results["steps_completed"].append(step.tool_name)
                else:
                    results["steps_failed"].append({
                        "tool": step.tool_name,
                        "error": result.get("error")
                    })

            except Exception as e:
                results["steps_failed"].append({
                    "tool": step.tool_name,
                    "error": str(e)
                })

        return results
```

### 3.3 å®ç°æ¡†æ¶

**æ¨èï¼šåŸºäºMCP SDK + LangGraph**

```python
from mcp import ClientSession, StdioServerParameters
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    user_query: str
    intent: str
    workflow: Workflow
    step_results: Annotated[list, operator.add]
    context: ConversationContext
    final_response: str

async def create_mcp_client(server_name: str):
    """åˆ›å»ºMCPå®¢æˆ·ç«¯è¿æ¥"""
    if server_name == "feedder":
        server_params = StdioServerParameters(
            command="feedder-mcp",
            args=["serve"]
        )
    elif server_name == "zotero":
        server_params = StdioServerParameters(
            command="zotero-mcp",
            args=["serve"]
        )
    elif server_name == "logseq":
        server_params = StdioServerParameters(
            command="logseq-mcp",
            args=["serve"]
        )

    client = ClientSession(server_params)
    await client.initialize()
    return client

async def understand_intent_node(state: AgentState) -> AgentState:
    """ç†è§£ç”¨æˆ·æ„å›¾"""
    query = state["user_query"]
    workflow = decompose_user_query(query)
    state["intent"] = workflow.intent.value
    state["workflow"] = workflow
    return state

async def plan_workflow_node(state: AgentState) -> AgentState:
    """è§„åˆ’å·¥ä½œæµ"""
    # å·²ç»åœ¨understand_intent_nodeä¸­å®Œæˆ
    return state

async def execute_tools_node(state: AgentState) -> AgentState:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    executor = MCPExecutor(state["context"])
    results = await executor.execute_workflow(state["workflow"])
    state["step_results"].append(results)
    return state

async def format_result_node(state: AgentState) -> AgentState:
    """æ ¼å¼åŒ–ç»“æœ"""
    results = state["step_results"][-1]
    response = format_workflow_results(results)
    state["final_response"] = response
    return state

def should_execute(state: AgentState) -> str:
    """å†³å®šæ˜¯å¦æ‰§è¡Œæˆ–éœ€è¦æ¾„æ¸…"""
    if state.get("workflow") and state["workflow"].steps:
        return "execute"
    else:
        return "clarify"

def build_master_agent():
    """æ„å»ºMaster Agent"""
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("understand_intent", understand_intent_node)
    workflow.add_node("plan_workflow", plan_workflow_node)
    workflow.add_node("execute_tools", execute_tools_node)
    workflow.add_node("format_result", format_result_node)

    # è®¾ç½®å…¥å£
    workflow.set_entry_point("understand_intent")

    # æ·»åŠ è¾¹
    workflow.add_edge("understand_intent", "plan_workflow")
    workflow.add_conditional_edges(
        "plan_workflow",
        should_execute,
        {
            "execute": "execute_tools",
            "clarify": "understand_intent"
        }
    )
    workflow.add_edge("execute_tools", "format_result")
    workflow.add_edge("format_result", END)

    return workflow.compile()
```

---

## 4. æ•°æ®æµä¸æ ¸å¿ƒå·¥ä½œæµ

### 4.1 æ•°æ®æµå‘å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RSSæº     â”‚  arXiv, Nature, Scienceç­‰
â”‚  Gmail      â”‚  è®ºæ–‡é€šçŸ¥é‚®ä»¶
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ feedder-mcp.fetch_feeds()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  feedder-mcp     â”‚
â”‚ - åŸå§‹æ•°æ®è·å–   â”‚
â”‚ - æ•°æ®æ¸…æ´—       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ filtered_papers
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  feedder-mcp     â”‚
â”‚ - å…³é”®è¯è¿‡æ»¤     â”‚
â”‚ - AIè¯­ä¹‰åŒ¹é…     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ matched_papers
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  feedder-mcp     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”¤ zotero-mcp   â”‚
â”‚ - å…ƒæ•°æ®å¢å¼º     â”‚  å»é‡  â”‚ - ç”¨æˆ·åº“     â”‚
â”‚ - å¯¼å‡ºå‡†å¤‡       â”‚  æ£€æŸ¥  â”‚ - å†å²æ•°æ®   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ enriched_papers
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  zotero-mcp      â”‚
â”‚ - å¯¼å…¥æ–‡çŒ®       â”‚
â”‚ - è‡ªåŠ¨æ ‡ç­¾       â”‚
â”‚ - PDFé™„ä»¶        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ sync event
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  logseq-mcp      â”‚
â”‚ - åˆ›å»ºé¡µé¢       â”‚
â”‚ - åº”ç”¨æ¨¡æ¿       â”‚
â”‚ - å»ºç«‹é“¾æ¥       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ ¸å¿ƒå·¥ä½œæµ

#### 4.2.1 å·¥ä½œæµ1ï¼šæ¯æ—¥è®ºæ–‡è‡ªåŠ¨å‘ç°

```yaml
è§¦å‘: Cron (æ¯å¤©æ—©ä¸Š8ç‚¹) æˆ– ç”¨æˆ·è¯·æ±‚

æ­¥éª¤:
  1. feedder_mcp.fetch_feeds()
     - è·å–æ‰€æœ‰RSSæºçš„æœ€æ–°æ¡ç›®
     - ä»Gmailè·å–è®ºæ–‡é€šçŸ¥é‚®ä»¶
     - è¾“å‡º: output/raw.json

  2. feedder_mcp.generate_keywords()
     - åŸºäºRESEARCH_PROMPTç”Ÿæˆå…´è¶£å…³é”®è¯
     - ä½¿ç”¨AIç”Ÿæˆç›¸å…³æœ¯è¯­

  3. zotero_mcp.get_metadata(limit=1000)
     - è·å–ç”¨æˆ·ç°æœ‰æ–‡çŒ®åº“å…ƒæ•°æ®
     - ç”¨äºå»é‡æ£€æŸ¥

  4. feedder_mcp.filter_papers()
     - å…³é”®è¯è¿‡æ»¤ï¼ˆORé€»è¾‘ï¼‰
     - AIè¯­ä¹‰è¿‡æ»¤ï¼ˆç›¸ä¼¼åº¦>é˜ˆå€¼ï¼‰
     - ä¸Zoteroåº“å¯¹æ¯”å»é‡

  5. feedder_mcp.enrich_papers()
     - CrossRefå…ƒæ•°æ®å¢å¼º
     - OpenAlexå¼•ç”¨ä¿¡æ¯
     - å¹¶å‘è¯·æ±‚ä¼˜åŒ–

  6. feedder_mcp.export_papers(format="zotero")
     - å¯¼å…¥åˆ°Zotero
     - è‡ªåŠ¨åº”ç”¨AIç”Ÿæˆçš„æ ‡ç­¾

  7. logseq_mcp.create_paper_page()
     - ä¸ºæ–°è®ºæ–‡åˆ›å»ºç¬”è®°é¡µé¢
     - åº”ç”¨å­¦æœ¯æ¨¡æ¿

  8. é€šçŸ¥ç”¨æˆ·
     - å‘é€æ‘˜è¦ï¼šå¯¼å…¥Nç¯‡è®ºæ–‡
     - åˆ—å‡ºtop 5æ¨è

é™çº§ç­–ç•¥:
  - å¦‚æœRSSå¤±è´¥ï¼Œå°è¯•Gmail
  - å¦‚æœAIè¿‡æ»¤å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯è¿‡æ»¤
  - å¦‚æœZoteroå¯¼å…¥å¤±è´¥ï¼Œä¿å­˜ä¸ºJSONå¤‡ä»½
```

#### 4.2.2 å·¥ä½œæµ2ï¼šæ–‡çŒ®æ·±åº¦åˆ†æ

```yaml
è§¦å‘: ç”¨æˆ·è¯·æ±‚"åˆ†æè¿™ç¯‡è®ºæ–‡"

æ­¥éª¤:
  1. zotero_mcp.get_fulltext(item_key)
     - è·å–PDFå…¨æ–‡å†…å®¹

  2. zotero_mcp.prepare_analysis()
     - å‡†å¤‡åˆ†æå·¥ä½œæµ
     - åˆ›å»ºå·¥ä½œæµID

  3. zotero_mcp.batch_analyze_pdfs()
     - ä½¿ç”¨LLMåˆ†æPDF
     - æå–ç»“æ„åŒ–ä¿¡æ¯ï¼š
       - ç ”ç©¶é—®é¢˜
       - æ ¸å¿ƒè´¡çŒ®
       - æ–¹æ³•è®º
       - ä¸»è¦å‘ç°
       - å±€é™æ€§

  4. logseq_mcp.insert_block()
     - å°†AIåˆ†æç»“æœå†™å…¥Logseqç¬”è®°
     - åœ¨"AIåˆ†æ"sectionæ·»åŠ å†…å®¹

  5. zotero_mcp.semantic_search()
     - åŸºäºåˆ†æç»“æœæœç´¢ç›¸å…³è®ºæ–‡
     - é™åˆ¶5ç¯‡æœ€ç›¸å…³

  6. logseq_mcp.insert_batch()
     - åœ¨ç¬”è®°ä¸­æ·»åŠ ç›¸å…³è®ºæ–‡é“¾æ¥
     - æ„å»ºçŸ¥è¯†ç½‘ç»œ

è¾“å‡ºæ ¼å¼:
  - åœ¨Logseqç¬”è®°é¡µé¢æ·»åŠ AIåˆ†æsection
  - åŒ…å«ç»“æ„åŒ–çš„åˆ†æç»“æœ
  - é“¾æ¥åˆ°ç›¸å…³è®ºæ–‡
```

#### 4.2.3 å·¥ä½œæµ3ï¼šä¸»é¢˜ç»¼è¿°ç”Ÿæˆ

```yaml
è§¦å‘: ç”¨æˆ·è¯·æ±‚"ç”ŸæˆXä¸»é¢˜çš„ç»¼è¿°"

æ­¥éª¤:
  1. feedder_mcp.generate_keywords(topic)
     - ç”Ÿæˆä¸»é¢˜ç›¸å…³å…³é”®è¯
     - è·å–ç›¸å…³æ¦‚å¿µå’Œæœ¯è¯­

  2. zotero_mcp.semantic_search(query=topic, limit=20)
     - è¯­ä¹‰æœç´¢ç›¸å…³æ–‡çŒ®

  3. zotero_mcp.get_fulltext_batch()
     - æ‰¹é‡è·å–å…¨æ–‡

  4. zotero_mcp.prepare_analysis()
     - å‡†å¤‡æ‰¹é‡åˆ†æ

  5. zotero_mcp.batch_analyze_pdfs()
     - å¹¶è¡Œåˆ†æå¤šç¯‡è®ºæ–‡
     - æå–å…³é”®ä¿¡æ¯

  6. zotero_mcp.synthesize_review()
     - ç»¼åˆåˆ†æç»“æœ
     - ç”Ÿæˆç»¼è¿°ç»“æ„ï¼š
       - å¼•è¨€
       - ä¸»è¦æ–¹æ³•åˆ†ç±»
       - å…³é”®å‘ç°æ€»ç»“
       - ç ”ç©¶gap
       - æœªæ¥æ–¹å‘

  7. logseq_mcp.create_review_page()
     - åˆ›å»ºç»¼è¿°é¡µé¢
     - å¼•ç”¨æ‰€æœ‰ç›¸å…³è®ºæ–‡
     - ç”Ÿæˆå¯è§†åŒ–å›¾è°±é“¾æ¥

è¾“å‡º:
  - Logseqç»¼è¿°é¡µé¢
  - ç»“æ„åŒ–çš„æ–‡çŒ®ç»¼è¿°
  - å¼•ç”¨æ–‡çŒ®çš„é“¾æ¥ç½‘ç»œ
```

#### 4.2.4 å·¥ä½œæµ4ï¼šçŸ¥è¯†æŸ¥è¯¢

```yaml
è§¦å‘: ç”¨æˆ·æå‡ºé—®é¢˜

æ­¥éª¤:
  1. zotero_mcp.semantic_search(query)
     - åœ¨æ–‡çŒ®åº“ä¸­è¯­ä¹‰æœç´¢
     - è¿”å›æœ€ç›¸å…³çš„10ç¯‡æ–‡çŒ®

  2. logseq_mcp.simple_query(query)
     - åœ¨ç¬”è®°å›¾è°±ä¸­æŸ¥è¯¢
     - è¿”å›ç›¸å…³é¡µé¢å’Œå—

  3. åˆå¹¶ç»“æœ
     - æŒ‰ç›¸å…³æ€§æ’åº
     - æä¾›æ‘˜è¦å’Œé“¾æ¥

  4. æ ¼å¼åŒ–è¾“å‡º
     - æ¸…æ™°å±•ç¤ºæ¥æºï¼ˆZotero/Logseqï¼‰
     - æä¾›å¿«é€Ÿè®¿é—®é“¾æ¥

ç‰¹æ®Šå¤„ç†:
  - å¦‚æœç”¨æˆ·æåˆ°"åˆšæ‰é‚£ç¯‡è®ºæ–‡"ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„active_item
  - å¦‚æœç”¨æˆ·æåˆ°ç‰¹å®šä½œè€…ï¼Œä¼˜å…ˆæœç´¢è¯¥ä½œè€…çš„å…¶ä»–ä½œå“
```

### 4.3 çŠ¶æ€ç®¡ç†

**Zoteroä½œä¸ºçŠ¶æ€æºï¼š**
- å·²å¯¼å…¥è®ºæ–‡çš„DOI/æ ‡é¢˜
- æ¯ç¯‡è®ºæ–‡çš„å¤„ç†çŠ¶æ€ï¼ˆæ–°å‘ç°ã€å·²åˆ†æã€å·²è¯»ï¼‰
- æ ‡ç­¾å’Œç¬”è®°çš„æ—¶é—´æˆ³

**åŒæ­¥çŠ¶æ€è¿½è¸ªï¼š**

```python
@dataclass
class SyncState:
    last_sync: datetime
    last_rss_fetch: datetime
    zotero_last_update: datetime
    logseq_last_sync: datetime
    vector_db_version: int

    def to_dict(self) -> dict:
        return {
            "last_sync": self.last_sync.isoformat(),
            "last_rss_fetch": self.last_rss_fetch.isoformat(),
            "zotero_last_update": self.zotero_last_update.isoformat(),
            "logseq_last_sync": self.logseq_last_sync.isoformat(),
            "vector_db_version": self.vector_db_version
        }

    @classmethod
    def from_dict(cls, data: dict) -> 'SyncState':
        return cls(
            last_sync=datetime.fromisoformat(data["last_sync"]),
            last_rss_fetch=datetime.fromisoformat(data["last_rss_fetch"]),
            zotero_last_update=datetime.fromisoformat(data["zotero_last_update"]),
            logseq_last_sync=datetime.fromisoformat(data["logseq_last_sync"]),
            vector_db_version=data["vector_db_version"]
        )

    def save(self, path: str):
        """ä¿å­˜åŒæ­¥çŠ¶æ€"""
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, indent=2)

    @classmethod
    def load(cls, path: str) -> 'SyncState':
        """åŠ è½½åŒæ­¥çŠ¶æ€"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return cls.from_dict(data)
```

---

## 5. é›†æˆå±‚è®¾è®¡

### 5.1 Zotero-LogseqåŒæ­¥é€‚é…å™¨

```python
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class ZoteroItem:
    item_key: str
    title: str
    authors: List[str]
    year: int
    doi: Optional[str]
    abstract: Optional[str]
    tags: List[str]
    date_added: str
    date_modified: str

@dataclass
class LogseqPage:
    page_name: str
    properties: Dict[str, Any]
    blocks: List[Dict]
    created_at: str
    updated_at: str

class ZoteroLogseqSyncAdapter:
    """Zoteroå’ŒLogseqä¹‹é—´çš„åŒæ­¥é€‚é…å™¨"""

    def __init__(self, zotero_client, logseq_client):
        self.zotero = zotero_client
        self.logseq = logseq_client
        self.template = self._load_academic_template()

    def _load_academic_template(self) -> str:
        """åŠ è½½å­¦æœ¯è®ºæ–‡æ¨¡æ¿"""
        return """
title:: [[{title}]]
authors:: {authors}
year:: {year}
doi:: {doi}
zotero_select:: zotero://select/items/{item_key}
zotero_key:: {item_key}
status:: to-read
tags:: {tags}

## æ‘˜è¦
{abstract}

## ç ”ç©¶é—®é¢˜
-
-
## æ–¹æ³•è®º
-
-
## ä¸»è¦å‘ç°
-
-
## AIåˆ†æ
ğŸ¤– *ç”±academic-agentç”Ÿæˆ*
- å…³é”®è´¡çŒ®ï¼š
- æ–¹æ³•åˆ›æ–°ç‚¹ï¼š
- ç›¸å…³æ–‡çŒ®ï¼š
## ä¸ªäººæ€è€ƒ
-
-
## ç›¸å…³æ–‡çŒ®
-
"""

    def sync_item_to_logseq(self, zotero_item: ZoteroItem) -> LogseqPage:
        """åŒæ­¥å•ä¸ªZoteroé¡¹ç›®åˆ°Logseq"""

        # æ£€æŸ¥é¡µé¢æ˜¯å¦å·²å­˜åœ¨
        page_name = self._generate_page_name(zotero_item)
        existing_page = self.logseq.get_page(page_name)

        if existing_page:
            # æ›´æ–°ç°æœ‰é¡µé¢
            return self._update_page(existing_page, zotero_item)
        else:
            # åˆ›å»ºæ–°é¡µé¢
            return self._create_page(zotero_item)

    def _generate_page_name(self, item: ZoteroItem) -> str:
        """ç”ŸæˆLogseqé¡µé¢å"""
        # æ ¼å¼: "Paper Title (Year)"
        return f"{item['title']} ({item['year']})"

    def _create_page(self, item: ZoteroItem) -> LogseqPage:
        """åˆ›å»ºæ–°çš„Logseqé¡µé¢"""

        # æ ¼å¼åŒ–ä½œè€…
        authors_str = ", ".join([
            f"[[{a}]]" for a in item.get("authors", [])
        ])

        # æ ¼å¼åŒ–æ ‡ç­¾
        tags_str = ", ".join([
            f"[[{t}]]" for t in item.get("tags", [])
        ])

        # åº”ç”¨æ¨¡æ¿
        content = self.template.format(
            title=item["title"],
            authors=authors_str,
            year=item.get("year", "n.d."),
            doi=item.get("doi", ""),
            item_key=item["item_key"],
            tags=tags_str,
            abstract=item.get("abstract", "")[:500] + "..."
            if item.get("abstract") else ""
        )

        # åˆ›å»ºé¡µé¢
        properties = {
            "title": item["title"],
            "authors": authors_str,
            "year": item.get("year", ""),
            "doi": item.get("doi", ""),
            "zotero_key": item["item_key"],
            "status": "to-read"
        }

        result = self.logseq.create_page(
            page_name=self._generate_page_name(item),
            properties=properties
        )

        # æ·»åŠ åˆå§‹å—
        self.logseq.insert_block(
            content=content.strip(),
            parent_block=page_name
        )

        return LogseqPage(
            page_name=self._generate_page_name(item),
            properties=properties,
            blocks=[{"content": content}],
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat()
        )

    def _update_page(self, existing_page: LogseqPage,
                     item: ZoteroItem) -> LogseqPage:
        """æ›´æ–°ç°æœ‰Logseqé¡µé¢"""

        # æ›´æ–°å±æ€§
        properties = existing_page.properties.copy()
        properties.update({
            "year": item.get("year", properties.get("year", "")),
            "doi": item.get("doi", properties.get("doi", "")),
            "zotero_key": item["item_key"]
        })

        # æ›´æ–°é¡µé¢
        self.logseq.update_page_properties(
            page_name=existing_page.page_name,
            properties=properties
        )

        return LogseqPage(
            page_name=existing_page.page_name,
            properties=properties,
            blocks=existing_page.blocks,
            created_at=existing_page.created_at,
            updated_at=datetime.now().isoformat()
        )

    def sync_batch(self, items: List[ZoteroItem]) -> List[LogseqPage]:
        """æ‰¹é‡åŒæ­¥Zoteroé¡¹ç›®åˆ°Logseq"""
        return [self.sync_item_to_logseq(item) for item in items]

    def find_page_by_zotero_key(self, item_key: str) -> Optional[LogseqPage]:
        """é€šè¿‡Zotero keyæŸ¥æ‰¾Logseqé¡µé¢"""

        # ä½¿ç”¨ç®€å•æŸ¥è¯¢
        query = f"zotero_key::{item_key}"
        result = self.logseq.simple_query(query)

        if result and result.get("pages"):
            return result["pages"][0]
        return None
```

### 5.2 å­¦æœ¯å·¥ä½œæµç¼–æ’å™¨

```python
from enum import Enum
from typing import Callable, Awaitable

class AcademicWorkflow(Enum):
    DAILY_PAPER_DISCOVERY = "daily_paper_discovery"
    PAPER_DEEP_ANALYSIS = "paper_deep_analysis"
    LITERATURE_REVIEW = "literature_review"
    KNOWLEDGE_QUERY = "knowledge_query"
    REFERENCE_MANAGEMENT = "reference_management"

class WorkflowOrchestrator:
    """å­¦æœ¯å·¥ä½œæµç¼–æ’å™¨"""

    def __init__(self, executor: MCPExecutor, context: ConversationContext):
        self.executor = executor
        self.context = context
        self.sync_adapter = ZoteroLogseqSyncAdapter(
            zotero_client=self.executor._execute_zotero,
            logseq_client=self.executor._execute_logseq
        )

    async def execute_workflow(self,
                               workflow: AcademicWorkflow,
                               params: dict) -> dict:
        """æ‰§è¡ŒæŒ‡å®šå·¥ä½œæµ"""

        handlers = {
            AcademicWorkflow.DAILY_PAPER_DISCOVERY: self._daily_discovery,
            AcademicWorkflow.PAPER_DEEP_ANALYSIS: self._deep_analysis,
            AcademicWorkflow.LITERATURE_REVIEW: self._literature_review,
            AcademicWorkflow.KNOWLEDGE_QUERY: self._knowledge_query,
            AcademicWorkflow.REFERENCE_MANAGEMENT: self._reference_management
        }

        handler = handlers.get(workflow)
        if not handler:
            raise ValueError(f"Unknown workflow: {workflow}")

        return await handler(params)

    async def _daily_discovery(self, params: dict) -> dict:
        """æ¯æ—¥è®ºæ–‡å‘ç°å·¥ä½œæµ"""

        results = {
            "papers_fetched": 0,
            "papers_filtered": 0,
            "papers_imported": 0,
            "logseq_pages_created": 0,
            "errors": []
        }

        try:
            # 1. è·å–RSSæº
            fetch_result = await self.executor.execute_tool(
                "feedder", "fetch_feeds",
                {"limit": params.get("limit", 200)}
            )
            results["papers_fetched"] = self._count_papers(fetch_result)

            # 2. è¿‡æ»¤è®ºæ–‡
            filter_result = await self.executor.execute_tool(
                "feedder", "filter_papers",
                {
                    "input_path": "output/raw.json",
                    "keywords": params.get("keywords", []),
                    "use_ai": True
                }
            )
            results["papers_filtered"] = self._count_papers(filter_result)

            # 3. å¢å¼ºå…ƒæ•°æ®
            enrich_result = await self.executor.execute_tool(
                "feedder", "enrich_papers",
                {
                    "input_path": "output/filtered.json",
                    "api": "all"
                }
            )

            # 4. å¯¼å‡ºåˆ°Zotero
            export_result = await self.executor.execute_tool(
                "feedder", "export_papers",
                {
                    "input_path": "output/enriched.json",
                    "format": "zotero"
                }
            )
            results["papers_imported"] = self._count_imported(export_result)

            # 5. åŒæ­¥åˆ°Logseq
            imported_items = self._extract_imported_items(export_result)
            for item in imported_items:
                try:
                    self.sync_adapter.sync_item_to_logseq(item)
                    results["logseq_pages_created"] += 1
                except Exception as e:
                    results["errors"].append(f"Logseq sync failed: {e}")

        except Exception as e:
            results["errors"].append(f"Workflow error: {e}")

        return results

    async def _deep_analysis(self, params: dict) -> dict:
        """æ·±åº¦åˆ†æå·¥ä½œæµ"""

        item_key = params.get("item_key")
        if not item_key:
            return {"error": "item_key is required"}

        results = {
            "analysis_complete": False,
            "logseq_updated": False,
            "related_papers_found": 0,
            "errors": []
        }

        try:
            # 1. å‡†å¤‡åˆ†æ
            prep_result = await self.executor.execute_tool(
                "zotero", "prepare_analysis",
                {"item_keys": [item_key]}
            )

            workflow_id = prep_result.get("workflow_id")

            # 2. æ‰§è¡Œåˆ†æ
            analyze_result = await self.executor.execute_tool(
                "zotero", "batch_analyze_pdfs",
                {
                    "workflow_id": workflow_id,
                    "model": params.get("model", "deepseek-chat")
                }
            )
            results["analysis_complete"] = True

            # 3. æ›´æ–°Logseq
            analysis = analyze_result.get("analysis", {})
            page = self.sync_adapter.find_page_by_zotero_key(item_key)
            if page:
                # æ›´æ–°AIåˆ†æsection
                self.logseq.update_block(
                    uuid=self._find_ai_analysis_block(page),
                    content=self._format_analysis(analysis)
                )
                results["logseq_updated"] = True

            # 4. æŸ¥æ‰¾ç›¸å…³è®ºæ–‡
            search_result = await self.executor.execute_tool(
                "zotero", "semantic_search",
                {"query": analysis.get("summary", ""), "limit": 5}
            )
            results["related_papers_found"] = len(search_result.get("results", []))

        except Exception as e:
            results["errors"].append(str(e))

        return results

    async def _literature_review(self, params: dict) -> dict:
        """æ–‡çŒ®ç»¼è¿°å·¥ä½œæµ"""
        # å®ç°ç±»ä¼¼...
        pass

    async def _knowledge_query(self, params: dict) -> dict:
        """çŸ¥è¯†æŸ¥è¯¢å·¥ä½œæµ"""
        # å®ç°ç±»ä¼¼...
        pass

    async def _reference_management(self, params: dict) -> dict:
        """æ–‡çŒ®ç®¡ç†å·¥ä½œæµ"""
        # å®ç°ç±»ä¼¼...
        pass

    def _count_papers(self, result: dict) -> int:
        """ä»ç»“æœä¸­ç»Ÿè®¡è®ºæ–‡æ•°é‡"""
        if result.get("status") == "success":
            return len(result.get("papers", []))
        return 0

    def _count_imported(self, result: dict) -> int:
        """ç»Ÿè®¡æˆåŠŸå¯¼å…¥çš„è®ºæ–‡æ•°é‡"""
        if result.get("status") == "success":
            return result.get("imported_count", 0)
        return 0

    def _extract_imported_items(self, result: dict) -> List[ZoteroItem]:
        """ä»å¯¼å…¥ç»“æœä¸­æå–é¡¹ç›®"""
        # å®ç°æå–é€»è¾‘
        pass

    def _find_ai_analysis_block(self, page: LogseqPage) -> str:
        """æŸ¥æ‰¾AIåˆ†æå—"""
        # å®ç°æŸ¥æ‰¾é€»è¾‘
        pass

    def _format_analysis(self, analysis: dict) -> str:
        """æ ¼å¼åŒ–åˆ†æç»“æœ"""
        return f"""
## AIåˆ†æ
ğŸ¤– *ç”±academic-agentç”Ÿæˆ*

### å…³é”®è´¡çŒ®
{chr(10).join(f"- {c}" for c in analysis.get('contributions', []))}

### æ–¹æ³•åˆ›æ–°ç‚¹
{chr(10).join(f"- {p}" for p in analysis.get('methodology', []))}

### ç›¸å…³æ–‡çŒ®æ¨è
{chr(10).join(f"- [[{r}]]" for r in analysis.get('related', []))}
"""
```

---

## 6. é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ

### 6.1 MCPæœåŠ¡å™¨çº§åˆ«çš„é”™è¯¯å¤„ç†

#### feedder-mcp

```python
# RSSè·å–å¤±è´¥
if not feed_data:
    return {
        "status": "error",
        "error_type": "feed_not_found",
        "message": "RSSæºæ— æ³•è®¿é—®",
        "suggestion": "è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¨åé‡è¯•",
        "fallback": "å¯ä»¥å°è¯•ä½¿ç”¨Gmailä½œä¸ºå¤‡é€‰æ•°æ®æº"
    }

# Gmail APIé™æµ
if rate_limit_exceeded:
    return {
        "status": "warning",
        "message": "Gmail APIé™æµï¼Œéƒ¨åˆ†é‚®ä»¶æœªè·å–",
        "retry_after": 3600,
        "partial_results": fetched_emails
    }

# AIè¿‡æ»¤å¤±è´¥
if ai_filter_error:
    return {
        "status": "partial",
        "message": "AIè¿‡æ»¤å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯è¿‡æ»¤",
        "fallback_results": keyword_filtered_results
    }
```

#### zotero-mcp

```python
# Zoteroæœªè¿è¡Œ
if not zotero_running():
    return {
        "status": "error",
        "error_type": "zotero_not_running",
        "message": "Zoteroæœªè¿è¡Œæˆ–æœ¬åœ°APIæœªå¯ç”¨",
        "solution": "è¯·åœ¨Zoteroåå¥½è®¾ç½®ä¸­å¯ç”¨HTTPæœåŠ¡å™¨"
    }

# æ–‡çŒ®ä¸å­˜åœ¨
if item_not_found:
    return {
        "status": "error",
        "error_type": "item_not_found",
        "item_key": item_key,
        "suggestion": "è¯¥æ–‡çŒ®å¯èƒ½å·²è¢«åˆ é™¤",
        "action": "æ£€æŸ¥å›æ”¶ç«™æˆ–é‡æ–°å¯¼å…¥"
    }

# è¯­ä¹‰æœç´¢æ•°æ®åº“æœªåˆå§‹åŒ–
if vector_db_not_initialized:
    return {
        "status": "error",
        "error_type": "vector_db_not_ready",
        "message": "è¯­ä¹‰æœç´¢æ•°æ®åº“æœªåˆå§‹åŒ–",
        "solution": "è¯·è¿è¡Œ 'zotero-mcp update-db' åˆå§‹åŒ–æ•°æ®åº“"
    }
```

#### logseq-mcp

```python
# Logseq APIæœªå¯åŠ¨
if not logseq_api_responsive():
    return {
        "status": "error",
        "error_type": "logseq_not_running",
        "message": "Logseqæœªè¿è¡Œæˆ–æœ¬åœ°APIæœªå¯ç”¨",
        "solution": "è¯·å¯åŠ¨Logseqå¹¶åœ¨è®¾ç½®ä¸­å¯ç”¨API (ç«¯å£12315)"
    }

# é¡µé¢å·²å­˜åœ¨
if page_exists(page_title):
    return {
        "status": "conflict",
        "error_type": "page_already_exists",
        "page_id": existing_page_id,
        "options": [
            "update_existing",
            "create_new_with_suffix",
            "skip"
        ],
        "default_action": "update_existing"
    }
```

### 6.2 Agentç¼–æ’çº§åˆ«çš„é”™è¯¯å¤„ç†

```python
class WorkflowErrorHandler:
    """å·¥ä½œæµé”™è¯¯å¤„ç†å™¨"""

    def __init__(self, context: ConversationContext):
        self.context = context

    async def handle_workflow_error(self,
                                     workflow: Workflow,
                                     step: WorkflowStep,
                                     error: Exception) -> dict:
        """å¤„ç†å·¥ä½œæµä¸­çš„é”™è¯¯"""

        error_info = {
            "workflow": workflow.description,
            "step": step.tool_name,
            "error_type": type(error).__name__,
            "error_message": str(error),
            "recovery_actions": []
        }

        # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ¢å¤ç­–ç•¥
        if isinstance(error, ConnectionError):
            # è¿æ¥é”™è¯¯ - å°è¯•é™çº§
            return await self._handle_connection_error(step, error_info)

        elif isinstance(error, TimeoutError):
            # è¶…æ—¶é”™è¯¯ - é‡è¯•æˆ–é™çº§
            return await self._handle_timeout_error(step, error_info)

        elif isinstance(error, ValueError):
            # å‚æ•°é”™è¯¯ - è¯·æ±‚ç”¨æˆ·è¾“å…¥
            return await self._handle_parameter_error(step, error_info)

        else:
            # æœªçŸ¥é”™è¯¯ - è®°å½•å¹¶ç»§ç»­
            return await self._handle_unknown_error(step, error_info)

    async def _handle_connection_error(self,
                                        step: WorkflowStep,
                                        error_info: dict) -> dict:
        """å¤„ç†è¿æ¥é”™è¯¯"""

        # å°è¯•é™çº§æ–¹æ¡ˆ
        fallbacks = {
            "feedder.fetch_feeds": "feedder.fetch_from_gmail",
            "zotero.semantic_search": "zotero.search",
            "logseq.create_page": None  # éå…³é”®ï¼Œå¯ä»¥è·³è¿‡
        }

        tool_path = f"{step.mcp_server}.{step.tool_name}"
        fallback_tool = fallbacks.get(tool_path)

        if fallback_tool:
            error_info["recovery_actions"].append({
                "action": "fallback",
                "to": fallback_tool,
                "reason": "è¿æ¥å¤±è´¥ï¼Œå°è¯•é™çº§æ–¹æ¡ˆ"
            })
        else:
            error_info["recovery_actions"].append({
                "action": "skip",
                "reason": "æ— å¯ç”¨é™çº§æ–¹æ¡ˆï¼Œè·³è¿‡æ­¤æ­¥éª¤"
            })

        return error_info

    async def _handle_timeout_error(self,
                                     step: WorkflowStep,
                                     error_info: dict) -> dict:
        """å¤„ç†è¶…æ—¶é”™è¯¯"""

        error_info["recovery_actions"].append({
            "action": "retry",
            "max_retries": 3,
            "backoff": "exponential"
        })

        return error_info

    async def _handle_parameter_error(self,
                                       step: WorkflowStep,
                                       error_info: dict) -> dict:
        """å¤„ç†å‚æ•°é”™è¯¯"""

        error_info["recovery_actions"].append({
            "action": "ask_user",
            "prompt": f"å‚æ•°é”™è¯¯ï¼Œè¯·æä¾›æ­£ç¡®çš„{step.tool_name}å‚æ•°",
            "required_params": step.parameters
        })

        return error_info

    async def _handle_unknown_error(self,
                                     step: WorkflowStep,
                                     error_info: dict) -> dict:
        """å¤„ç†æœªçŸ¥é”™è¯¯"""

        error_info["recovery_actions"].append({
            "action": "log_and_continue",
            "log_level": "error",
            "message": f"æœªçŸ¥é”™è¯¯: {error_info['error_message']}"
        })

        return error_info

    def should_continue_workflow(self, error_info: dict) -> bool:
        """åˆ¤æ–­å·¥ä½œæµæ˜¯å¦åº”è¯¥ç»§ç»­"""

        critical_steps = [
            "feedder.fetch_feeds",
            "zotero.get_metadata"
        ]

        tool_path = f"{error_info['step']}.{error_info['step']}"

        # å¦‚æœæ˜¯å…³é”®æ­¥éª¤å¤±è´¥ï¼Œä¸ç»§ç»­
        if tool_path in critical_steps:
            return False

        # æ£€æŸ¥æ¢å¤åŠ¨ä½œ
        for action in error_info.get("recovery_actions", []):
            if action.get("action") in ["fallback", "retry"]:
                return True

        return False
```

### 6.3 æ•°æ®ä¸€è‡´æ€§ä¿è¯

```python
class ConflictResolver:
    """å†²çªè§£å†³å™¨"""

    def resolve_sync_conflict(self,
                               zotero_item: ZoteroItem,
                               logseq_page: LogseqPage) -> dict:
        """è§£å†³Zoteroå’ŒLogseqä¹‹é—´çš„åŒæ­¥å†²çª"""

        zotero_ts = datetime.fromisoformat(zotero_item.date_modified)
        logseq_ts = datetime.fromisoformat(logseq_page.updated_at)

        if zotero_ts > logseq_ts:
            # Zoteroæ›´æ–°ï¼Œè¦†ç›–Logseq
            return {
                "action": "sync_zotero_to_logseq",
                "reason": "Zoteroå…ƒæ•°æ®æ›´æ–°",
                "timestamp": zotero_ts.isoformat()
            }

        elif logseq_ts > zotero_ts:
            # Logseqæ›´æ–°ï¼Œä¿ç•™å‰¯æœ¬
            return {
                "action": "create_conflict_copy",
                "reason": "Logseqç¬”è®°æœ‰ç”¨æˆ·ä¿®æ”¹",
                "timestamp": logseq_ts.isoformat(),
                "backup_page": f"{logseq_page.page_name}_backup"
            }

        else:
            # åŒæ—¶æ›´æ–°ï¼Œæ™ºèƒ½åˆå¹¶
            return {
                "action": "merge_changes",
                "reason": "åŒæ—¶æ›´æ–°",
                "strategy": "zotero_metadata_priority"
            }

    def merge_item_and_page(self,
                            zotero_item: ZoteroItem,
                            logseq_page: LogseqPage) -> LogseqPage:
        """æ™ºèƒ½åˆå¹¶Zoteroé¡¹ç›®å’ŒLogseqé¡µé¢"""

        # Zoteroå…ƒæ•°æ®ä¼˜å…ˆ
        merged_properties = logseq_page.properties.copy()
        merged_properties.update({
            "title": zotero_item.title,
            "authors": zotero_item.authors,
            "year": zotero_item.year,
            "doi": zotero_item.doi,
            "zotero_key": zotero_item.item_key
        })

        # ä¿ç•™Logseqçš„ä¿®æ”¹
        user_sections = ["ä¸ªäººæ€è€ƒ", "ç›¸å…³æ–‡çŒ®", "ç¬”è®°"]
        preserved_blocks = [
            block for block in logseq_page.blocks
            if any(section in block.get("content", "")
                   for section in user_sections)
        ]

        return LogseqPage(
            page_name=logseq_page.page_name,
            properties=merged_properties,
            blocks=preserved_blocks,
            created_at=logseq_page.created_at,
            updated_at=datetime.now().isoformat()
        )
```

---

## 7. æµ‹è¯•ç­–ç•¥

### 7.1 å•å…ƒæµ‹è¯•

```python
# test_workflows.py
import pytest
from academic_agent.orchestrator import WorkflowOrchestrator
from academic_agent.models import Intent, Workflow

class TestWorkflowDecomposition:
    """æµ‹è¯•å·¥ä½œæµåˆ†è§£"""

    def test_paper_discovery_intent(self):
        """æµ‹è¯•è®ºæ–‡å‘ç°æ„å›¾è¯†åˆ«"""
        query = "å¸®æˆ‘æ‰¾æœ€è¿‘ä¸€å‘¨å…³äºtransformerçš„è®ºæ–‡"
        workflow = decompose_user_query(query)

        assert workflow.intent == Intent.PAPER_DISCOVERY
        assert len(workflow.steps) > 0
        assert workflow.steps[0].tool_name == "fetch_feeds"

    def test_paper_analysis_intent(self):
        """æµ‹è¯•è®ºæ–‡åˆ†ææ„å›¾è¯†åˆ«"""
        query = "åˆ†æè¿™ç¯‡è®ºæ–‡ABCD1234"
        workflow = decompose_user_query(query)

        assert workflow.intent == Intent.PAPER_ANALYSIS
        assert any(s.tool_name == "batch_analyze_pdfs"
                   for s in workflow.steps)

    def test_literature_review_intent(self):
        """æµ‹è¯•æ–‡çŒ®ç»¼è¿°æ„å›¾è¯†åˆ«"""
        query = "ç”Ÿæˆæ·±åº¦å­¦ä¹ åœ¨NLPé¢†åŸŸçš„ç»¼è¿°"
        workflow = decompose_user_query(query)

        assert workflow.intent == Intent.LITERATURE_REVIEW

# test_context.py
class TestConversationContext:
    """æµ‹è¯•å¯¹è¯ä¸Šä¸‹æ–‡"""

    def test_remember_and_recall(self):
        """æµ‹è¯•è®°å¿†å’Œå¬å›"""
        ctx = ConversationContext()
        ctx.remember("research_interests", ["AI", "NLP"])

        interests = ctx.recall("research_interests")
        assert interests == ["AI", "NLP"]

    def test_active_items(self):
        """æµ‹è¯•æ´»è·ƒæ–‡çŒ®ç®¡ç†"""
        ctx = ConversationContext()
        ctx.add_active_item("KEY123", {"title": "Test Paper"})

        item = ctx.get_active_item("KEY123")
        assert item["metadata"]["title"] == "Test Paper"

    def test_workflow_history(self):
        """æµ‹è¯•å·¥ä½œæµå†å²"""
        ctx = ConversationContext()
        workflow = Workflow(
            intent=Intent.PAPER_DISCOVERY,
            description="Test",
            steps=[]
        )
        ctx.add_workflow(workflow)

        assert len(ctx.workflow_history) == 1

# test_sync_adapter.py
class TestSyncAdapter:
    """æµ‹è¯•åŒæ­¥é€‚é…å™¨"""

    def test_page_name_generation(self):
        """æµ‹è¯•é¡µé¢åç”Ÿæˆ"""
        adapter = ZoteroLogseqSyncAdapter(None, None)
        item = ZoteroItem(
            item_key="KEY123",
            title="Test Paper",
            authors=["Author One"],
            year=2024,
            doi="10.1234/test",
            abstract="Abstract",
            tags=["AI"],
            date_added="2024-01-01",
            date_modified="2024-01-01"
        )

        page_name = adapter._generate_page_name(item)
        assert page_name == "Test Paper (2024)"

    def test_template_application(self):
        """æµ‹è¯•æ¨¡æ¿åº”ç”¨"""
        adapter = ZoteroLogseqSyncAdapter(None, None)
        template = adapter.template

        # æµ‹è¯•æ¨¡æ¿å˜é‡æ›¿æ¢
        content = template.format(
            title="Test",
            authors="[[Author]]",
            year="2024",
            doi="10.1234/test",
            item_key="KEY123",
            tags="[[AI]]",
            abstract="Test abstract"
        )

        assert "title:: [[Test]]" in content
        assert "zotero_key:: KEY123" in content
```

### 7.2 é›†æˆæµ‹è¯•

```python
# test_integration.py
import pytest
from academic_agent.orchestrator import WorkflowOrchestrator, AcademicWorkflow
from academic_agent.context import ConversationContext

@pytest.mark.integration
class TestEndToEndWorkflows:
    """ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•"""

    @pytest.fixture
    def orchestrator(self):
        """åˆ›å»ºç¼–æ’å™¨å®ä¾‹"""
        context = ConversationContext()
        executor = MCPExecutor(context)
        return WorkflowOrchestrator(executor, context)

    async def test_daily_discovery_workflow(self, orchestrator):
        """æµ‹è¯•æ¯æ—¥è®ºæ–‡å‘ç°å·¥ä½œæµ"""
        # æ³¨æ„ï¼šéœ€è¦å®é™…çš„MCPæœåŠ¡å™¨è¿è¡Œ

        params = {
            "limit": 50,
            "keywords": ["transformer", "attention"]
        }

        results = await orchestrator.execute_workflow(
            AcademicWorkflow.DAILY_PAPER_DISCOVERY,
            params
        )

        assert results["papers_fetched"] >= 0
        assert results["papers_filtered"] >= 0
        assert "errors" in results

    async def test_deep_analysis_workflow(self, orchestrator):
        """æµ‹è¯•æ·±åº¦åˆ†æå·¥ä½œæµ"""
        # éœ€è¦ä¸€ä¸ªæµ‹è¯•ç”¨çš„item_key
        params = {
            "item_key": "TEST_ITEM_KEY",
            "model": "deepseek-chat"
        }

        results = await orchestrator.execute_workflow(
            AcademicWorkflow.PAPER_DEEP_ANALYSIS,
            params
        )

        # æ ¹æ®æ˜¯å¦æœ‰è¯¥æ–‡çŒ®ï¼Œç»“æœå¯èƒ½ä¸åŒ
        assert "analysis_complete" in results or "error" in results

    async def test_knowledge_query_workflow(self, orchestrator):
        """æµ‹è¯•çŸ¥è¯†æŸ¥è¯¢å·¥ä½œæµ"""
        params = {
            "query": "attention mechanism in neural networks"
        }

        results = await orchestrator.execute_workflow(
            AcademicWorkflow.KNOWLEDGE_QUERY,
            params
        )

        assert "zotero_results" in results or "logseq_results" in results
```

### 7.3 æ€§èƒ½æµ‹è¯•

```python
# test_performance.py
import pytest
import time
from academic_agent.orchestrator import WorkflowOrchestrator

@pytest.mark.performance
class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""

    async def test_large_scale_semantic_search(self):
        """æµ‹è¯•å¤§è§„æ¨¡è¯­ä¹‰æœç´¢æ€§èƒ½"""
        # å‡è®¾æœ‰10000ç¯‡æ–‡çŒ®çš„åº“
        start_time = time.time()

        # æ‰§è¡Œæœç´¢
        result = await zotero_semantic_search("deep learning", limit=20)

        duration = time.time() - start_time

        # æ–­è¨€ï¼šæœç´¢åº”åœ¨2ç§’å†…å®Œæˆ
        assert duration < 2.0
        assert len(result.get("results", [])) <= 20

    async def test_batch_analysis_performance(self):
        """æµ‹è¯•æ‰¹é‡åˆ†ææ€§èƒ½"""
        papers = ["ITEM_" + str(i) for i in range(10)]

        start_time = time.time()
        results = await batch_analyze_pdfs(papers)
        duration = time.time() - start_time

        # æ‰¹é‡åˆ†æ10ç¯‡è®ºæ–‡åº”åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        assert duration < 300  # 5åˆ†é’Ÿ
        assert len(results) == len(papers)

    def test_context_serialization_performance(self):
        """æµ‹è¯•ä¸Šä¸‹æ–‡åºåˆ—åŒ–æ€§èƒ½"""
        ctx = ConversationContext()
        # æ·»åŠ å¤§é‡æ•°æ®
        for i in range(1000):
            ctx.add_active_item(f"KEY_{i}", {"title": f"Paper {i}"})

        start_time = time.time()
        data = ctx.to_dict()
        duration = time.time() - start_time

        # åºåˆ—åŒ–åº”åœ¨100mså†…å®Œæˆ
        assert duration < 0.1
        assert len(data["active_items"]) == 1000
```

---

## 8. éƒ¨ç½²ä¸é…ç½®

### 8.1 é¡¹ç›®ç»“æ„

```
academic-agent/
â”œâ”€â”€ .gitmodules              # Gitå­æ¨¡å—é…ç½®
â”œâ”€â”€ .env.example             # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ README.md                # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ pyproject.toml           # Pythoné¡¹ç›®é…ç½®
â”œâ”€â”€ feedder-mcp/             # å­æ¨¡å—
â”œâ”€â”€ logseq-mcp/              # å­æ¨¡å—
â”œâ”€â”€ zotero-mcp/              # å­æ¨¡å—
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # CLIå…¥å£
â”‚   â”œâ”€â”€ server.py            # MCPæœåŠ¡å™¨å…¥å£
â”‚   â”œâ”€â”€ orchestrator/        # Master Agentç¼–æ’å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ intent.py        # æ„å›¾è¯†åˆ«
â”‚   â”‚   â”œâ”€â”€ workflow.py      # å·¥ä½œæµå®šä¹‰
â”‚   â”‚   â”œâ”€â”€ executor.py      # MCPå·¥å…·æ‰§è¡Œå™¨
â”‚   â”‚   â””â”€â”€ context.py       # ä¸Šä¸‹æ–‡ç®¡ç†
â”‚   â”œâ”€â”€ integration/         # é›†æˆå±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sync_adapter.py  # Zotero-LogseqåŒæ­¥
â”‚   â”‚   â””â”€â”€ templates.py     # å­¦æœ¯æ¨¡æ¿
â”‚   â”œâ”€â”€ workflows/           # é¢„å®šä¹‰å·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ discovery.py     # è®ºæ–‡å‘ç°
â”‚   â”‚   â”œâ”€â”€ analysis.py      # è®ºæ–‡åˆ†æ
â”‚   â”‚   â””â”€â”€ review.py        # æ–‡çŒ®ç»¼è¿°
â”‚   â””â”€â”€ models/              # æ•°æ®æ¨¡å‹
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ intent.py
â”‚       â”œâ”€â”€ workflow.py
â”‚       â””â”€â”€ context.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_workflows.py
â”‚   â”œâ”€â”€ test_context.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â””â”€â”€ test_performance.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ design/
â”‚   â”‚   â””â”€â”€ 2025-02-08-academic-agent-design.md
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ mcp-tools.md
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ academic_template.md  # Logseqå­¦æœ¯æ¨¡æ¿
â”‚   â””â”€â”€ workflows.yaml        # å·¥ä½œæµé…ç½®
â””â”€â”€ scripts/
    â”œâ”€â”€ setup.sh              # åˆå§‹åŒ–è„šæœ¬
    â”œâ”€â”€ install.sh            # å®‰è£…è„šæœ¬
    â””â”€â”€ test.sh               # æµ‹è¯•è„šæœ¬
```

### 8.2 ç¯å¢ƒé…ç½®

**.env.example:**

```bash
# ====================
# academic-agenté…ç½®
# ====================

# --------------------
# Zoteroé…ç½®
# --------------------
ZOTERO_LOCAL=true
ZOTERO_API_KEY=your_zotero_api_key
ZOTERO_LIBRARY_ID=your_library_id
ZOTERO_LIBRARY_TYPE=user

# --------------------
# Logseqé…ç½®
# --------------------
LOGSEQ_GRAPH_PATH=~/Documents/Logseq/MyGraph
LOGSEQ_API_ENDPOINT=http://localhost:12315
LOGSEQ_API_TOKEN=your_logseq_api_token
LOGSEQ_ENABLE_ADVANCED_QUERIES=true
LOGSEQ_ENABLE_GIT_OPERATIONS=false

# --------------------
# AIæœåŠ¡é…ç½®
# --------------------
OPENAI_API_KEY=sk-...
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

ANTHROPIC_API_KEY=sk-ant-...
DEFAULT_CHAT_MODEL=gpt-4o
DEFAULT_EMBEDDING_MODEL=default

# DeepSeek (ç”¨äºPDFåˆ†æ)
DEEPSEEK_API_KEY=your_deepseek_key
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# --------------------
# å‘é‡æ•°æ®åº“
# --------------------
VECTOR_DB_PATH=~/.academic-agent/vectordb
ZOTERO_EMBEDDING_MODEL=default

# --------------------
# Gmailé…ç½®ï¼ˆå¯é€‰ï¼‰
# --------------------
GMAIL_ENABLED=false
GMAIL_LABELS=Arxiv,ScienceDirect,NatureAlerts
GMAIL_TOKEN_JSON=
GMAIL_CREDENTIALS_JSON=

# --------------------
# RSSé…ç½®
# --------------------
FEEDDER_MCP_OPML=configs/feeds/academic.opml
RESEARCH_PROMPT=My research interests include deep learning, natural language processing, and attention mechanisms.

# --------------------
# academic-agentç‰¹å®šé…ç½®
# --------------------
ACADEMIC_AGENT_AUTO_SYNC=true
ACADEMIC_AGENT_DEFAULT_TEMPLATE=configs/academic_template.md
ACADEMIC_AGENT_MAX_PAPERS_PER_IMPORT=50
ACADEMIC_AGENT_CONTEXT_PATH=~/.academic-agent/context.json
```

### 8.3 å®‰è£…æ–¹å¼

**æœ¬åœ°å¼€å‘å®‰è£…:**

```bash
# 1. å…‹éš†ä»“åº“ï¼ˆåŒ…å«å­æ¨¡å—ï¼‰
git clone --recurse-submodules https://github.com/your-org/academic-agent.git
cd academic-agent

# å¦‚æœå·²ç»å…‹éš†ä½†æ²¡æœ‰å­æ¨¡å—ï¼š
git submodule update --init --recursive

# 2. å®‰è£…ä¾èµ–
# ä½¿ç”¨uv
uv sync

# æˆ–ä½¿ç”¨pip
pip install -e .

# 3. å®‰è£…å­æ¨¡å—ä¾èµ–
cd feedder-mcp && uv sync && cd ..
cd logseq-mcp && uv sync && cd ..
cd zotero-mcp && uv sync && cd ..

# 4. é…ç½®ç¯å¢ƒ
cp .env.example ~/.academic-agent.env
# ç¼–è¾‘ ~/.academic-agent.env å¡«å…¥ä½ çš„APIå¯†é’¥

# 5. åˆå§‹åŒ–
academic-agent init --interactive

# 6. å¯åŠ¨MCPæœåŠ¡å™¨
academic-agent serve
```

**Dockeréƒ¨ç½²:**

```yaml
# docker-compose.yml
version: '3.8'

services:
  academic-agent:
    build: .
    container_name: academic-agent
    env_file:
      - .env
    volumes:
      - ~/.academic-agent:/app/data
      - ~/Documents/Logseq/MyGraph:/logseq-graph
    ports:
      - "8080:8080"
    depends_on:
      - chromadb

  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    volumes:
      - ~/.academic-agent/vectordb:/chroma/chroma
    ports:
      - "8000:8000"
```

**Claude Desktopé…ç½®:**

```json
{
  "mcpServers": {
    "academic-agent": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/academic-agent",
        "run",
        "academic-agent",
        "serve"
      ],
      "env": {
        "ZOTERO_LOCAL": "true",
        "LOGSEQ_API_TOKEN": "your_token",
        "OPENAI_API_KEY": "sk-..."
      }
    }
  }
}
```

---

## 9. å¼€å‘è·¯çº¿å›¾

### Phase 0: å‡†å¤‡é˜¶æ®µ - 1å‘¨

**Week 1: ç¯å¢ƒæ­å»º**
- [x] ç¡®è®¤å­æ¨¡å—å·²é…ç½®ï¼ˆfeedder-mcp, logseq-mcp, zotero-mcpï¼‰
- [ ] è®¾ç½®å¼€å‘ç¯å¢ƒå’Œå·¥å…·é“¾
- [ ] åˆ›å»ºé¡¹ç›®ç»“æ„
- [ ] ç¼–å†™åŸºç¡€é…ç½®æ–‡ä»¶

### Phase 1: Master Agentæ ¸å¿ƒ - 3å‘¨

**Week 2: æ„å›¾è¯†åˆ«ä¸å·¥ä½œæµå®šä¹‰**
- [ ] å®ç°`decompose_user_query`å‡½æ•°
- [ ] å®šä¹‰æ ¸å¿ƒå·¥ä½œæµï¼ˆå‘ç°ã€åˆ†æã€ç»¼è¿°ã€æŸ¥è¯¢ï¼‰
- [ ] å®ç°`ConversationContext`ç±»
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–

**Week 3: MCPå·¥å…·æ‰§è¡Œå™¨**
- [ ] å®ç°`MCPExecutor`ç±»
- [ ] å®ç°ä¸ä¸‰ä¸ªå­æ¨¡å—çš„é€šä¿¡
- [ ] é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
- [ ] é›†æˆæµ‹è¯•

**Week 4: åŸºç¡€å·¥ä½œæµå®ç°**
- [ ] å®ç°æ¯æ—¥è®ºæ–‡å‘ç°å·¥ä½œæµ
- [ ] å®ç°çŸ¥è¯†æŸ¥è¯¢å·¥ä½œæµ
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•

### Phase 2: é›†æˆå±‚ - 2å‘¨

**Week 5: Zotero-LogseqåŒæ­¥**
- [ ] å®ç°`ZoteroLogseqSyncAdapter`
- [ ] å­¦æœ¯è®ºæ–‡æ¨¡æ¿è®¾è®¡
- [ ] æ‰¹é‡åŒæ­¥åŠŸèƒ½
- [ ] å†²çªè§£å†³æœºåˆ¶

**Week 6: é«˜çº§å·¥ä½œæµ**
- [ ] å®ç°æ·±åº¦åˆ†æå·¥ä½œæµ
- [ ] å®ç°æ–‡çŒ®ç»¼è¿°å·¥ä½œæµ
- [ ] å·¥ä½œæµç¼–æ’å™¨

### Phase 3: å¢å¼ºåŠŸèƒ½ - 2å‘¨

**Week 7: CLIå’Œç”¨æˆ·ç•Œé¢**
- [ ] å®ç°CLIå‘½ä»¤
- [ ] äº¤äº’å¼é…ç½®å‘å¯¼
- [ ] ä¸Šä¸‹æ–‡æŒä¹…åŒ–

**Week 8: æ€§èƒ½ä¼˜åŒ–**
- [ ] å¹¶è¡Œå¤„ç†ä¼˜åŒ–
- [ ] ç¼“å­˜æœºåˆ¶
- [ ] æ€§èƒ½æµ‹è¯•

### Phase 4: ç”Ÿäº§åŒ– - 2å‘¨

**Week 9: ç¨³å®šæ€§**
- [ ] å®Œæ•´é”™è¯¯å¤„ç†
- [ ] æ—¥å¿—ç³»ç»Ÿ
- [ ] æµ‹è¯•è¦†ç›–ç‡>80%

**Week 10: éƒ¨ç½²ä¸æ–‡æ¡£**
- [ ] Dockeræ”¯æŒ
- [ ] ç”¨æˆ·æ–‡æ¡£
- [ ] APIæ–‡æ¡£
- [ ] å‘å¸ƒå‡†å¤‡

---

## 10. å…³é”®æŠ€æœ¯ä¸å®ç°è¦ç‚¹

### 10.1 MCPå­æ¨¡å—é›†æˆ

**å­æ¨¡å—åˆå§‹åŒ–:**

```bash
# æ·»åŠ å­æ¨¡å—
git submodule add https://github.com/your-org/feedder-mcp.git feedder-mcp
git submodule add https://github.com/dailydaniel/logseq-mcp.git logseq-mcp
git submodule add https://github.com/liuchzzyy/zotero-mcp.git zotero-mcp

# æ›´æ–°å­æ¨¡å—
git submodule update --remote --merge

# å…‹éš†æ—¶åŒ…å«å­æ¨¡å—
git clone --recurse-submodules https://github.com/your-org/academic-agent.git
```

**å­æ¨¡å—ç‰ˆæœ¬ç®¡ç†:**

```bash
# é”å®šç‰¹å®šç‰ˆæœ¬
cd feedder-mcp
git checkout v2.2.0
cd ..

cd logseq-mcp
git checkout v2.1.1
cd ..

cd zotero-mcp
git checkout v2.2.0
cd ..

# æäº¤å­æ¨¡å—æ›´æ–°
git add feedder-mcp logseq-mcp zotero-mcp
git commit -m "Update submodules to specific versions"
```

### 10.2 Master Agentæ¶æ„

**åŸºäºMCP SDKçš„é€šä¿¡:**

```python
from mcp import ClientSession, StdioServerParameters
import asyncio

async def create_mcp_session(server_name: str,
                              server_path: str) -> ClientSession:
    """åˆ›å»ºMCPä¼šè¯"""

    server_params = StdioServerParameters(
        command="uv",
        args=[
            "--directory", server_path,
            "run", f"{server_name}-mcp"
        ]
    )

    session = ClientSession(server_params)
    await session.__aenter__()
    await session.initialize()

    return session

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    feedder_session = await create_mcp_session(
        "feedder",
        "../feedder-mcp"
    )

    # è°ƒç”¨å·¥å…·
    result = await feedder_session.call_tool(
        "fetch_feeds",
        arguments={"limit": 100}
    )

    print(result)

    await feedder_session.__aexit__(None, None, None)

asyncio.run(main())
```

### 10.3 å­¦æœ¯æ¨¡æ¿è®¾è®¡

**Logseqå­¦æœ¯è®ºæ–‡æ¨¡æ¿:**

```markdown
---
title:: [[{title}]]
authors:: {authors_formatted}
year:: {year}
doi:: {doi}
zotero_select:: zotero://select/items/{item_key}
zotero_key:: {item_key}
publication:: {publication}
volume:: {volume}
issue:: {issue}
pages:: {pages}
status:: to-read
tags:: {tags_formatted}
type:: paper
created:: {created_date}
---

## æ‘˜è¦
{abstract}

## ç ”ç©¶é—®é¢˜
-
-
## æ–¹æ³•è®º
-
-
## ä¸»è¦å‘ç°
-
-
## å±€é™æ€§
-
-
## AIåˆ†æ
ğŸ¤– *ç”±academic-agentç”Ÿæˆ*
- å…³é”®è´¡çŒ®ï¼š
- æ–¹æ³•åˆ›æ–°ç‚¹ï¼š
- ç›¸å…³æ–‡çŒ®æ¨èï¼š
## ä¸ªäººæ€è€ƒ
-
-
## ç¬”è®°
-
-
## å¼•ç”¨æœ¬æ–‡
```

### 10.4 LangGraphå·¥ä½œæµç¼–æ’

**å®šä¹‰çŠ¶æ€å›¾:**

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    query: str
    intent: str
    context: ConversationContext
    workflow: Workflow
    step_index: int
    results: Annotated[list, operator.add]
    final_response: str

def create_master_agent_graph():
    """åˆ›å»ºMaster AgentçŠ¶æ€å›¾"""

    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("parse_intent", parse_intent_node)
    workflow.add_node("plan_steps", plan_steps_node)
    workflow.add_node("execute_step", execute_step_node)
    workflow.add_node("check_completion", check_completion_node)
    workflow.add_node("format_response", format_response_node)

    # å®šä¹‰è¾¹
    workflow.set_entry_point("parse_intent")
    workflow.add_edge("parse_intent", "plan_steps")
    workflow.add_edge("plan_steps", "execute_step")
    workflow.add_edge("execute_step", "check_completion")

    # æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "check_completion",
        should_continue,
        {
            "continue": "execute_step",
            "format": "format_response"
        }
    )
    workflow.add_edge("format_response", END)

    return workflow.compile()
```

---

## 11. æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶

### 11.1 APIè°ƒç”¨ä¼˜åŒ–

**æ‰¹é‡å¤„ç†ç­–ç•¥:**

```python
class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self, batch_size: int = 10):
        self.batch_size = batch_size

    async def batch_embed_texts(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡embedæ–‡æœ¬"""
        embeddings = []

        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            response = await openai.embeddings.create(
                model="text-embedding-3-small",
                input=batch
            )
            embeddings.extend([e.embedding for e in response.data])

        return embeddings

    async def batch_analyze_papers(self, item_keys: List[str]) -> List[dict]:
        """æ‰¹é‡åˆ†æè®ºæ–‡"""
        results = []

        for i in range(0, len(item_keys), self.batch_size):
            batch = item_keys[i:i + self.batch_size]
            # å¹¶å‘å¤„ç†
            tasks = [self._analyze_single(key) for key in batch]
            batch_results = await asyncio.gather(*tasks)
            results.extend(batch_results)

        return results
```

**ç¼“å­˜æœºåˆ¶:**

```python
from functools import lru_cache
import hashlib
import pickle

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = "~/.academic-agent/cache"):
        self.cache_dir = Path(cache_dir).expanduser()
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get(self, key: str, max_age: int = 86400) -> Any:
        """è·å–ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.cache"

        if not cache_file.exists():
            return None

        # æ£€æŸ¥å¹´é¾„
        age = time.time() - cache_file.stat().st_mtime
        if age > max_age:
            return None

        with open(cache_file, 'rb') as f:
            return pickle.load(f)

    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.cache"
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)

    def hash_content(self, content: str) -> str:
        """å“ˆå¸Œå†…å®¹"""
        return hashlib.md5(content.encode()).hexdigest()

# ä½¿ç”¨ç¤ºä¾‹
cache = CacheManager()

def get_embedding_with_cache(text: str) -> List[float]:
    """å¸¦ç¼“å­˜çš„embed"""
    cache_key = cache.hash_content(text)
    cached = cache.get(f"embedding_{cache_key}")

    if cached:
        return cached

    # è®¡ç®—embed
    embedding = openai.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

    # ç¼“å­˜
    cache.set(f"embedding_{cache_key}", embedding)
    return embedding
```

### 11.2 æˆæœ¬ä¼°ç®—

**æ¯æœˆæˆæœ¬ä¼°ç®—ï¼ˆå‡è®¾ï¼‰:**

| æ“ä½œ | æ¬¡æ•° | å•æ¬¡æˆæœ¬ | æœˆæˆæœ¬ |
|------|------|----------|--------|
| è¯­ä¹‰æœç´¢ï¼ˆ1000ç¯‡ç´¢å¼•ï¼‰ | 1æ¬¡/æœˆ | $0.0001/ç¯‡ | $0.10 |
| æ—¥å¸¸è¯­ä¹‰æŸ¥è¯¢ | 100æ¬¡/æœˆ | $0.0001/æ¬¡ | $0.01 |
| PDFæ·±åº¦åˆ†æ | 20ç¯‡/æœˆ | $0.05/ç¯‡ | $1.00 |
| å…³é”®è¯ç”Ÿæˆ | 30æ¬¡/æœˆ | $0.001/æ¬¡ | $0.03 |
| æ‰¹æ³¨æå– | 50æ¬¡/æœˆ | $0.0005/æ¬¡ | $0.025 |
| **æ€»è®¡** | | | **çº¦$1.20/æœˆ** |

**ä¼˜åŒ–å»ºè®®:**
- ä½¿ç”¨æœ¬åœ°embedæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
- æ‰¹é‡å¤„ç†é™ä½APIè°ƒç”¨
- æ™ºèƒ½ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
- æ ¹æ®ä½¿ç”¨é¢‘ç‡è°ƒæ•´æ›´æ–°ç­–ç•¥

### 11.3 æ€§èƒ½æŒ‡æ ‡

**ç›®æ ‡æ€§èƒ½:**
- è®ºæ–‡å‘ç°å·¥ä½œæµ: < 5åˆ†é’Ÿï¼ˆ200ç¯‡è®ºæ–‡ï¼‰
- æ·±åº¦åˆ†æå·¥ä½œæµ: < 3åˆ†é’Ÿ/ç¯‡
- è¯­ä¹‰æœç´¢: < 2ç§’ï¼ˆ10,000ç¯‡åº“ï¼‰
- çŸ¥è¯†æŸ¥è¯¢: < 1ç§’

**ç›‘æ§æŒ‡æ ‡:**
```python
@dataclass
class PerformanceMetrics:
    workflow_name: str
    start_time: datetime
    end_time: datetime
    steps_completed: int
    steps_failed: int
    api_calls_made: int
    cache_hits: int
    cache_misses: int

    @property
    def duration(self) -> float:
        return (self.end_time - self.start_time).total_seconds()

    @property
    def success_rate(self) -> float:
        total = self.steps_completed + self.steps_failed
        return self.steps_completed / total if total > 0 else 0

    @property
    def cache_hit_rate(self) -> float:
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0
```

---

## æ€»ç»“

### v2.0 ä¸»è¦æ›´æ–°

**æ¶æ„å˜åŒ–:**
- âœ… ä¸‰ä¸ªMCPæœåŠ¡å™¨å·²ä½œä¸ºå­æ¨¡å—å­˜åœ¨ï¼ˆfeedder-mcp, logseq-mcp, zotero-mcpï¼‰
- âœ… ä¸»è¦å¼€å‘å·¥ä½œè½¬å‘Master Agentå’Œé›†æˆå±‚
- âœ… é‡æ–°è®¾è®¡å¼€å‘è·¯çº¿å›¾ï¼Œèšç„¦é›†æˆè€Œéä»é›¶å¼€å‘

**æ ¸å¿ƒåŠŸèƒ½:**
- âœ… è®ºæ–‡å‘ç°ä¸å¯¼å…¥ï¼ˆåŸºäºfeedder-mcpï¼‰
- âœ… æ–‡çŒ®æ·±åº¦åˆ†æï¼ˆåŸºäºzotero-mcpï¼‰
- âœ… Zotero-LogseqåŒå‘åŒæ­¥
- âœ… è¯­ä¹‰æœç´¢å’ŒçŸ¥è¯†æŸ¥è¯¢
- âœ… ä¸»é¢˜ç»¼è¿°ç”Ÿæˆ

**æŠ€æœ¯æ ˆ:**
- âœ… MCPåè®®å’Œå­æ¨¡å—æ¶æ„
- âœ… ç°æœ‰MCPæœåŠ¡å™¨ï¼ˆæ— éœ€é‡æ–°å¼€å‘ï¼‰
- âœ… Master Agentï¼ˆLangGraph + MCP SDKï¼‰
- âœ… é›†æˆå±‚å’Œå­¦æœ¯å·¥ä½œæµ

**å®æ–½è®¡åˆ’:**
- Phase 0 (1å‘¨): å‡†å¤‡é˜¶æ®µ
- Phase 1 (3å‘¨): Master Agentæ ¸å¿ƒ
- Phase 2 (2å‘¨): é›†æˆå±‚
- Phase 3 (2å‘¨): å¢å¼ºåŠŸèƒ½
- Phase 4 (2å‘¨): ç”Ÿäº§åŒ–

**æ€»è®¡: çº¦10å‘¨å®ŒæˆMVP**

---

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨:**
1. åˆå§‹åŒ–å­æ¨¡å—
2. æ­å»ºMaster AgentåŸºç¡€æ¡†æ¶
3. å®ç°ç¬¬ä¸€ä¸ªå·¥ä½œæµï¼ˆè®ºæ–‡å‘ç°ï¼‰
4. ç¼–å†™é›†æˆæµ‹è¯•
5. è¿­ä»£æ”¹è¿›
