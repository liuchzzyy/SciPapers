# å­¦æœ¯Agentç³»ç»Ÿè®¾è®¡æ–‡æ¡£

**æ—¥æœŸ**: 2025-02-08
**ç‰ˆæœ¬**: 1.0
**çŠ¶æ€**: è®¾è®¡é˜¶æ®µ

---

## ç›®å½•

1. [ç³»ç»Ÿæ¶æ„æ¦‚è¿°](#1-ç³»ç»Ÿæ¶æ„æ¦‚è¿°)
2. [Master Agentä¸å·¥å…·ç¼–æ’](#2-master-agentä¸å·¥å…·ç¼–æ’)
3. [MCPæœåŠ¡å™¨è¯¦ç»†è®¾è®¡](#3-mcpæœåŠ¡å™¨è¯¦ç»†è®¾è®¡)
4. [æ•°æ®æµä¸æ ¸å¿ƒå·¥ä½œæµ](#4-æ•°æ®æµä¸æ ¸å¿ƒå·¥ä½œæµ)
5. [é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ](#5-é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ)
6. [æµ‹è¯•ç­–ç•¥](#6-æµ‹è¯•ç­–ç•¥)
7. [éƒ¨ç½²ä¸é…ç½®](#7-éƒ¨ç½²ä¸é…ç½®)
8. [å¼€å‘è·¯çº¿å›¾](#8-å¼€å‘è·¯çº¿å›¾)
9. [å…³é”®æŠ€æœ¯ä¸å®ç°è¦ç‚¹](#9-å…³é”®æŠ€æœ¯ä¸å®ç°è¦ç‚¹)
10. [æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶](#10-æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶)

---

## 1. ç³»ç»Ÿæ¶æ„æ¦‚è¿°

### 1.1 æ ¸å¿ƒè®¾è®¡ç†å¿µ

è¿™æ˜¯ä¸€ä¸ªåŸºäºMCPï¼ˆModel Context Protocolï¼‰çš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿï¼Œé€šè¿‡ç»Ÿä¸€çš„Master Agentç¼–æ’å››ä¸ªä¸“é—¨çš„MCPæœåŠ¡å™¨ã€‚

**æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š**
- **å•ä¸€å…¥å£**ï¼šç”¨æˆ·åªä¸"å­¦æœ¯åŠ©æ‰‹"å¯¹è¯ï¼Œä¸éœ€è¦ç†è§£åº•å±‚MCPæ¶æ„
- **Zoteroä¸ºä¸­å¿ƒ**ï¼šZoteroæ˜¯æ–‡çŒ®ç®¡ç†çš„å”¯ä¸€çœŸç›¸æºï¼ŒLogseqä½œä¸ºè§†å›¾å±‚
- **äº‘AIé©±åŠ¨**ï¼šæ‰€æœ‰AIåŠŸèƒ½ä½¿ç”¨å•†ä¸šAPIï¼ˆOpenAI/Anthropicï¼‰ç¡®ä¿æ•ˆæœ
- **ç”¨æˆ·æ§åˆ¶**ï¼šRSSæºå’ŒGmailæ ‡ç­¾å®Œå…¨ç”±ç”¨æˆ·é…ç½®ï¼ŒAgentä¸è‡ªåŠ¨æ¨æ–­

### 1.2 ç³»ç»Ÿåˆ†å±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    äº¤äº’å±‚                                â”‚
â”‚              Master Agent (ç”¨æˆ·å¯¹è¯)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç¼–æ’å±‚                                â”‚
â”‚          æ„å›¾è¯†åˆ« â†’ ä»»åŠ¡åˆ†è§£ â†’ å·¥å…·è°ƒç”¨                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æœåŠ¡å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚paper_feederâ”‚ â”‚  zotero  â”‚ â”‚  logseq  â”‚ â”‚    ai    â”‚  â”‚
â”‚  â”‚   _mcp   â”‚ â”‚   _mcp   â”‚ â”‚   _mcp   â”‚ â”‚   _mcp   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Zotero  â”‚ â”‚  Logseq  â”‚ â”‚ChromaDB  â”‚                â”‚
â”‚  â”‚ (æ–‡çŒ®åº“) â”‚ â”‚ (ç¬”è®°)   â”‚ â”‚(å‘é‡ç´¢å¼•)â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 å…¸å‹å·¥ä½œæµç¤ºä¾‹

**ç”¨æˆ·è¯·æ±‚**: "å¸®æˆ‘æŠŠæœ€è¿‘ä¸€å‘¨arXivä¸Šå…³äºtransformerçš„è®ºæ–‡å¯¼å…¥æˆ‘çš„æ–‡çŒ®åº“"

```
1. Master Agentè§£ææ„å›¾
   â”œâ”€ è¯†åˆ«: æ–‡çŒ®å‘ç°ä»»åŠ¡
   â”œâ”€ æ—¶é—´èŒƒå›´: æœ€è¿‘ä¸€å‘¨
   â””â”€ å…³é”®è¯: transformer

2. è°ƒç”¨paper_feeder_mcp.get_rss_feeds()
   â”œâ”€ è·å–arXiv RSSæº
   â””â”€ è¿”å›åŸå§‹è®ºæ–‡åˆ—è¡¨

3. è°ƒç”¨ai_mcp.semantic_match()
   â”œâ”€ ä½¿ç”¨transformerçš„embedding
   â”œâ”€ åŒ¹é…ç›¸å…³è®ºæ–‡
   â””â”€ è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°

4. è°ƒç”¨zotero_mcp.batch_import()
   â”œâ”€ å¯¼å…¥é«˜ç›¸å…³åº¦è®ºæ–‡
   â”œâ”€ è‡ªåŠ¨æ·»åŠ æ ‡ç­¾
   â””â”€ ä¸‹è½½PDFé™„ä»¶

5. è°ƒç”¨logseq_mcp.sync_from_zotero()
   â”œâ”€ ä¸ºæ–°è®ºæ–‡åˆ›å»ºç¬”è®°é¡µé¢
   â”œâ”€ åº”ç”¨æ¨¡æ¿ç»“æ„
   â””â”€ å»ºç«‹Zoteroé“¾æ¥

6. è¿”å›ç»“æœç»™ç”¨æˆ·
   â””â”€ "å·²å¯¼å…¥15ç¯‡è®ºæ–‡ï¼Œå·²åˆ›å»ºLogseqç¬”è®°"
```

---

## 2. Master Agentä¸å·¥å…·ç¼–æ’

### 2.1 Master Agentçš„èŒè´£

Master Agentæ˜¯ç³»ç»Ÿçš„åè°ƒè€…ï¼Œè´Ÿè´£ç†è§£ç”¨æˆ·æ„å›¾å¹¶è°ƒç”¨åˆé€‚çš„MCPå·¥å…·ç»„åˆã€‚å®ƒä¸æ˜¯ç®€å•çš„å·¥å…·è·¯ç”±å™¨ï¼Œè€Œæ˜¯éœ€è¦ç†è§£å­¦æœ¯å·¥ä½œæµçš„çŸ¥è¯†ã€‚

### 2.2 æ ¸å¿ƒèƒ½åŠ›

#### 2.2.1 æ„å›¾è¯†åˆ«ä¸ä»»åŠ¡åˆ†è§£

```python
def decompose_user_query(query: str) -> Workflow:
    """å°†ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºMCPå·¥å…·è°ƒç”¨åºåˆ—"""

    # ç¤ºä¾‹1: æ–‡çŒ®å‘ç°
    if "æ‰¾è®ºæ–‡" in query or "æ–°è®ºæ–‡" in query:
        return Workflow([
            ("paper_feeder_mcp", "fetch_feeds"),
            ("ai_mcp", "generate_keywords"),
            ("ai_mcp", "semantic_match"),
            ("zotero_mcp", "batch_import"),
            ("logseq_mcp", "sync_from_zotero")
        ])

    # ç¤ºä¾‹2: æ–‡çŒ®åˆ†æ
    elif "åˆ†æè®ºæ–‡" in query or "ç†è§£è¿™ç¯‡" in query:
        return Workflow([
            ("zotero_mcp", "get_item_fulltext"),
            ("ai_mcp", "analyze_pdf"),
            ("logseq_mcp", "update_page_block"),
            ("ai_mcp", "find_similar_papers"),
            ("logseq_mcp", "add_linked_pages")
        ])
```

#### 2.2.2 ä¸Šä¸‹æ–‡ç®¡ç†

```python
class ConversationContext:
    """ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡"""

    def __init__(self):
        self.user_profile = {
            "research_interests": [],
            "recent_queries": [],
            "zotero_library_snapshot": None
        }
        self.active_items = {}  # å½“å‰è®¨è®ºçš„æ–‡çŒ®
        self.workflow_history = []

    def remember(self, key, value):
        """è®°ä½ä¿¡æ¯ä¾›åç»­ä½¿ç”¨"""
        self.user_profile[key] = value

    def recall(self, key):
        """å¬å›ä¹‹å‰çš„ä¿¡æ¯"""
        return self.user_profile.get(key)

# ä½¿ç”¨ç¤ºä¾‹
ctx.remember("last_discussed_paper", "ABCD1234")
# ç”¨æˆ·é—®: "åˆšæ‰é‚£ç¯‡è®ºæ–‡çš„ä½œè€…è¿˜æœ‰ä»€ä¹ˆå…¶ä»–å·¥ä½œï¼Ÿ"
paper_key = ctx.recall("last_discussed_paper")
```

#### 2.2.3 å·¥å…·é€‰æ‹©ç­–ç•¥

```python
TOOL_MAPPING = {
    "æ–‡çŒ®å‘ç°": {
        "tools": [
            "paper_feeder_mcp.fetch_feeds",
            "ai_mcp.generate_keywords",
            "ai_mcp.semantic_match"
        ]
    },
    "æ–‡çŒ®ç®¡ç†": {
        "tools": [
            "zotero_mcp.search_items",
            "zotero_mcp.get_annotations",
            "logseq_mcp.sync_from_zotero"
        ]
    },
    "å†…å®¹åˆ†æ": {
        "tools": [
            "zotero_mcp.get_fulltext",
            "ai_mcp.analyze_pdf",
            "ai_mcp.semantic_search"
        ]
    }
}

def select_tools(intent: str) -> list[str]:
    """æ ¹æ®æ„å›¾é€‰æ‹©å·¥å…·"""
    return TOOL_MAPPING.get(intent, {}).get("tools", [])
```

### 2.3 å®ç°æ¡†æ¶é€‰æ‹©

**æ¨èï¼šLangGraph**

```python
from langgraph.graph import StateGraph
from typing import TypedDict

class AgentState(TypedDict):
    user_query: str
    intent: str
    tools_to_call: list
    context: dict
    final_result: str

def build_master_agent():
    workflow = StateGraph(AgentState)

    # å®šä¹‰èŠ‚ç‚¹
    workflow.add_node("understand_intent", understand_intent_node)
    workflow.add_node("plan_workflow", plan_workflow_node)
    workflow.add_node("execute_tools", execute_tools_node)
    workflow.add_node("format_result", format_result_node)

    # å®šä¹‰æ‰§è¡Œæµç¨‹
    workflow.set_entry_point("understand_intent")
    workflow.add_edge("understand_intent", "plan_workflow")
    workflow.add_conditional_edges(
        "plan_workflow",
        should_execute,
        {
            "execute": "execute_tools",
            "clarify": "understand_intent"
        }
    )
    workflow.add_edge("execute_tools", "format_result")
    workflow.set_finish_point("format_result")

    return workflow.compile()
```

---

## 3. MCPæœåŠ¡å™¨è¯¦ç»†è®¾è®¡

### 3.1 paper_feeder_mcpï¼šè®ºæ–‡æ•°æ®è·å–ä¸æ¸…æ´—

#### 3.1.1 æ ¸å¿ƒèŒè´£

ä»RSSæºå’ŒGmailè·å–è®ºæ–‡å…ƒæ•°æ®ï¼Œæ¸…æ´—æ ‡å‡†åŒ–åä¾›å…¶ä»–ç»„ä»¶ä½¿ç”¨ã€‚

#### 3.1.2 ä¸»è¦å·¥å…·

```python
@mcp.tool
def list_feeds() -> str:
    """åˆ—å‡ºæ‰€æœ‰é…ç½®çš„RSSæº"""

@mcp.tool
def fetch_feed(feed_url: str, limit: int = 50) -> str:
    """è·å–æŒ‡å®šRSSçš„æœ€æ–°æ¡ç›®"""

@mcp.tool
def add_feed(feed_url: str, category: str) -> str:
    """æ·»åŠ æ–°çš„RSSæºåˆ°é…ç½®"""

@mcp.tool
def fetch_papers_from_gmail(label: str, days: int = 7) -> str:
    """ä»GmailæŒ‡å®šæ ‡ç­¾è·å–å­¦æœ¯é‚®ä»¶"""

@mcp.tool
def normalize_paper_metadata(raw_data: List[Dict]) -> str:
    """æ ‡å‡†åŒ–è®ºæ–‡å…ƒæ•°æ®"""

@mcp.tool
def deduplicate_papers(papers: List[Dict], zotero_library: str) -> str:
    """ä¸Zoteroåº“å¯¹æ¯”ï¼Œå»é™¤å·²å­˜åœ¨çš„è®ºæ–‡"""
```

#### 3.1.3 æ•°æ®æµ

```
RSS/Gmail â†’ åŸå§‹æ•°æ® â†’ normalize_paper_metadata()
â†’ deduplicate_papers(å¯¹æ¯”Zotero) â†’ æ¸…æ´—åçš„è®ºæ–‡åˆ—è¡¨
â†’ ä¼ é€’ç»™AI_mcpè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
```

#### 3.1.4 é…ç½®æ–‡ä»¶

```json
{
  "rss_feeds": [
    {
      "url": "http://export.arxiv.org/rss/cs.AI",
      "category": "AI",
      "enabled": true
    },
    {
      "url": "http://export.arxiv.org/rss/cs.CV",
      "category": "CV",
      "enabled": true
    },
    {
      "url": "https://www.nature.com/nrsa.rss",
      "category": "Robotics",
      "enabled": false
    }
  ],
  "gmail": {
    "enabled": true,
    "labels": ["Arxiv", "ScienceDirect", "NatureAlerts"],
    "poll_interval_hours": 6
  }
}
```

---

### 3.2 zotero_mcpï¼šæ–‡çŒ®ç®¡ç†æ ¸å¿ƒ

#### 3.2.1 æ ¸å¿ƒèŒè´£

ä½œä¸ºç³»ç»Ÿçš„æ–‡çŒ®ç®¡ç†çœŸç›¸æºï¼Œæä¾›æ–‡çŒ®çš„CRUDæ“ä½œã€æ ‡ç­¾ç®¡ç†ã€å…¨æ–‡æ£€ç´¢ç­‰æ ¸å¿ƒèƒ½åŠ›ã€‚

#### 3.2.2 è®¾è®¡ç­–ç•¥

åŸºäºç°æœ‰çš„ [zotero-mcp](https://github.com/54yyyu/zotero-mcp) é¡¹ç›®è¿›è¡Œæ‰©å±•ï¼Œå¤ç”¨å…¶æˆç†ŸåŠŸèƒ½ï¼Œé‡ç‚¹å¢å¼ºä¸Logseqå’ŒAI_mcpçš„é›†æˆã€‚

#### 3.2.3 æ ¸å¿ƒå·¥å…·ï¼ˆå¤ç”¨zotero-mcpï¼‰

```python
@mcp.tool
def search_items(query: str, tag: List[str] = None, limit: int = 10) -> str:
    """å…³é”®è¯æœç´¢æ–‡çŒ®"""

@mcp.tool
def semantic_search(query: str, limit: int = 10) -> str:
    """AIè¯­ä¹‰æœç´¢ï¼ˆåŸºäºå‘é‡æ•°æ®åº“ï¼‰"""

@mcp.tool
def get_recent_items(limit: int = 20) -> str:
    """è·å–æœ€è¿‘æ·»åŠ çš„æ–‡çŒ®"""

@mcp.tool
def get_item_fulltext(item_key: str) -> str:
    """è·å–æ–‡çŒ®å…¨æ–‡å†…å®¹"""

@mcp.tool
def get_annotations(item_key: str, use_pdf_extraction: bool = True) -> str:
    """è·å–PDFæ‰¹æ³¨"""

@mcp.tool
def get_notes(item_key: str = None, limit: int = 20) -> str:
    """è·å–æ–‡çŒ®ç¬”è®°"""
```

#### 3.2.4 éœ€è¦æ‰©å±•çš„å·¥å…·

```python
@mcp.tool
def export_items_for_logseq(item_keys: List[str], format: str = "markdown") -> str:
    """å¯¼å‡ºæ–‡çŒ®å…ƒæ•°æ®ä¸ºLogseqæ ¼å¼

    è¿”å›æ ¼å¼ï¼š
    - title:: è®ºæ–‡æ ‡é¢˜
    - authors:: ä½œè€…åˆ—è¡¨
    - year:: å¹´ä»½
    - doi:: DOI
    - abstract:: æ‘˜è¦
    - zotero_select:: zotero://select/items/KEY
    """

@mcp.tool
def get_items_updated_since(timestamp: str) -> str:
    """è·å–æŒ‡å®šæ—¶é—´åæ›´æ–°çš„æ–‡çŒ®ï¼ˆç”¨äºLogseqå¢é‡åŒæ­¥ï¼‰"""

@mcp.tool
def batch_update_tags_from_logseq(tag_updates: List[Dict]) -> str:
    """æ‰¹é‡æ›´æ–°æ ‡ç­¾ï¼ˆä»LogseqåŒæ­¥å›æ¥ï¼‰

    æ ¼å¼ï¼š[{"item_key": "xxx", "add_tags": ["tag1"], "remove_tags": ["tag2"]}]
    """
```

#### 3.2.5 é…ç½®æ–‡ä»¶

```json
{
  "zotero": {
    "local": true,
    "library_type": "user",
    "api_key": "${ZOTERO_API_KEY}",
    "library_id": "${ZOTERO_LIBRARY_ID}"
  },
  "semantic_search": {
    "embedding_model": "openai",
    "openai_api_key": "${OPENAI_API_KEY}",
    "embedding_model_name": "text-embedding-3-small",
    "auto_update": true,
    "update_frequency": "daily"
  }
}
```

---

### 3.3 logseq_mcpï¼šçŸ¥è¯†å›¾è°±ä¸ç¬”è®°ç®¡ç†

#### 3.3.1 æ ¸å¿ƒèŒè´£

ä½œä¸ºç³»ç»Ÿçš„çŸ¥è¯†ç®¡ç†è§†å›¾å±‚ï¼Œä»ZoteroåŒæ­¥æ–‡çŒ®æ•°æ®ï¼Œæä¾›ç¬”è®°çš„CRUDã€æ ‡ç­¾ç®¡ç†ã€å›¾è°±æŸ¥è¯¢ç­‰åŠŸèƒ½ã€‚

#### 3.3.2 ä¸»è¦å·¥å…·

```python
@mcp.tool
def create_paper_page(item_key: str, zotero_data: Dict) -> str:
    """ä¸ºZoteroæ–‡çŒ®åˆ›å»ºLogseqé¡µé¢"""

@mcp.tool
def get_page(page_id: str) -> str:
    """è·å–é¡µé¢å†…å®¹å’Œå±æ€§"""

@mcp.tool
def update_page_block(block_id: str, content: str) -> str:
    """æ›´æ–°é¡µé¢å—å†…å®¹"""

@mcp.tool
def query_graph(query: str, query_type: str = "page") -> str:
    """æŸ¥è¯¢å›¾è°±ï¼šæŒ‰æ ‡ç­¾ã€å±æ€§ã€é“¾æ¥å…³ç³»"""

@mcp.tool
def get_linked_pages(page_id: str) -> str:
    """è·å–é“¾æ¥åˆ°çš„å…¶ä»–é¡µé¢ï¼ˆçŸ¥è¯†ç½‘ç»œï¼‰"""

@mcp.tool
def add_tag_to_block(block_id: str, tag: str) -> str:
    """ä¸ºå—æ·»åŠ æ ‡ç­¾"""

@mcp.tool
def sync_from_zotero(items: List[Dict], sync_mode: str = "incremental") -> str:
    """ä»ZoteroåŒæ­¥æ–‡çŒ®åˆ°Logseq"""

@mcp.tool
def get_zotero_backlinks(item_key: str) -> str:
    """è·å–å¼•ç”¨è¯¥æ–‡çŒ®çš„Logseqç¬”è®°é¡µé¢"""
```

#### 3.3.3 Logseqé¡µé¢æ¨¡æ¿

```markdown
title:: [[Deep Learning for NLP]]
authors:: [[Bengio, Y.]], [[Goodfellow, I.]]
year:: 2016
doi:: 10.1234/example
zotero_select:: zotero://select/items/ABCD1234
zotero_key:: ABCD1234
status:: reading
tags:: [[NLP]], [[Deep Learning]], [[Survey]]

## æ‘˜è¦
æœ¬æ–‡ç»¼è¿°äº†æ·±åº¦å­¦ä¹ åœ¨NLPé¢†åŸŸçš„åº”ç”¨...

## ç ”ç©¶é—®é¢˜
- å¦‚ä½•ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºè¯­è¨€ï¼Ÿ
- å“ªäº›æ¶æ„æœ€é€‚åˆåºåˆ—å»ºæ¨¡ï¼Ÿ

## æ–¹æ³•è®º
- Word2Vec embeddings
- RNN/LSTMæ¶æ„
- Attentionæœºåˆ¶

## ä¸»è¦å‘ç°
- Embeddingsæ•è·è¯­ä¹‰å…³ç³»
- LSTMè§£å†³é•¿è·ç¦»ä¾èµ–
- Attentionæ˜¾è‘—æå‡æ€§èƒ½

## ä¸ªäººæ€è€ƒ
- è¿™ç¯‡è®ºæ–‡çš„å±€é™æ˜¯ä»€ä¹ˆï¼Ÿ
- ä¸å…¶ä»–ç ”ç©¶çš„è”ç³»ï¼š[[Transformer Architecture]]
- æœªæ¥ç ”ç©¶æ–¹å‘ï¼šé¢„è®­ç»ƒæ¨¡å‹

## AIåˆ†æ
ğŸ¤– *ç”±AI_mcpç”Ÿæˆ*
- å…³é”®è´¡çŒ®ï¼šxxx
- æ–¹æ³•åˆ›æ–°ç‚¹ï¼šxxx
- ç›¸å…³æ–‡çŒ®æ¨èï¼š[[Attention Is All You Need]], [[BERT]]
```

#### 3.3.4 é…ç½®æ–‡ä»¶

```json
{
  "logseq": {
    "graph_path": "~/Documents/Logseq/MyGraph",
    "api_endpoint": "http://localhost:12315",
    "sync": {
      "auto_sync": true,
      "sync_interval_minutes": 30,
      "default_template": "academic_paper"
    }
  }
}
```

---

### 3.4 ai_mcpï¼šæ™ºèƒ½åˆ†æä¸è¯­ä¹‰ç†è§£

#### 3.4.1 æ ¸å¿ƒèŒè´£

æä¾›æ‰€æœ‰AIé©±åŠ¨çš„æ™ºèƒ½åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¯­ä¹‰æœç´¢ã€å…³é”®å­—ç”Ÿæˆã€PDFæ·±åº¦åˆ†æã€‚

#### 3.4.2 æŠ€æœ¯æ ˆ

- **Embedding**: OpenAI text-embedding-3-small
- **ç”Ÿæˆ/åˆ†æ**: GPT-4o / Claude 3.5 Sonnet
- **å‘é‡æ•°æ®åº“**: ChromaDBï¼ˆæœ¬åœ°æŒä¹…åŒ–ï¼‰

#### 3.4.3 ä¸»è¦å·¥å…·

**è¯­ä¹‰æœç´¢å·¥å…·**

```python
@mcp.tool
def semantic_search(
    query: str,
    search_scope: str = "all",
    limit: int = 10,
    filters: Dict = None
) -> str:
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢æ–‡çŒ®å’Œç¬”è®°"""

@mcp.tool
def find_similar_papers(item_key: str, limit: int = 5) -> str:
    """æŸ¥æ‰¾ä¸æŒ‡å®šè®ºæ–‡è¯­ä¹‰ç›¸ä¼¼çš„å…¶ä»–è®ºæ–‡"""

@mcp.tool
def build_semantic_index(force_rebuild: bool = False) -> str:
    """ä¸ºZoteroåº“æ„å»ºè¯­ä¹‰ç´¢å¼•"""
```

**å…³é”®å­—ç”Ÿæˆå·¥å…·**

```python
@mcp.tool
def generate_search_keywords(
    topic: str,
    context: str = "",
    num_keywords: int = 10
) -> str:
    """ä¸ºç ”ç©¶ä¸»é¢˜ç”Ÿæˆæœç´¢å…³é”®å­—

    è¿”å›ï¼š
    {
      "primary_keywords": ["transformer", "attention"],
      "secondary_keywords": ["self-attention", "multi-head"],
      "related_topics": ["BERT", "GPT", "seq2seq"],
      "search_queries": ["transformer architecture", "attention mechanism"]
    }
    """

@mcp.tool
def classify_paper_categories(paper_text: str, categories: List[str]) -> str:
    """å°†è®ºæ–‡åˆ†ç±»åˆ°æŒ‡å®šç±»åˆ«ï¼ˆç”¨äºRSSè¿‡æ»¤ï¼‰"""
```

**PDFæ·±åº¦åˆ†æå·¥å…·**

```python
@mcp.tool
def analyze_pdf(
    item_key: str,
    analysis_type: str = "comprehensive",
    focus_areas: List[str] = None
) -> str:
    """æ·±åº¦åˆ†æPDFå†…å®¹

    analysis_typeé€‰é¡¹ï¼š
    - comprehensive: å…¨æ–‡åˆ†æ
    - methodology: ä»…åˆ†ææ–¹æ³•è®º
    - findings: ä»…åˆ†æä¸»è¦å‘ç°
    - critique: æ‰¹åˆ¤æ€§åˆ†æï¼ˆå±€é™ã€æ¼æ´ï¼‰

    è¿”å›ç»“æ„åŒ–åˆ†æï¼š
    {
      "summary": "è®ºæ–‡æ€»ç»“",
      "key_contributions": ["è´¡çŒ®1", "è´¡çŒ®2"],
      "methodology": "æ–¹æ³•è®ºåˆ†æ",
      "main_findings": ["å‘ç°1", "å‘ç°2"],
      "limitations": "å±€é™æ€§ä¸æ‰¹è¯„",
      "future_work": "æœªæ¥æ–¹å‘",
      "related_work": "ç›¸å…³å·¥ä½œ",
      "suggested_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
    }
    """

@mcp.tool
def extract_paper_structure(pdf_content: str) -> str:
    """æå–è®ºæ–‡ç»“æ„åŒ–å†…å®¹"""

@mcp.tool
def compare_papers(item_keys: List[str]) -> str:
    """å¯¹æ¯”å¤šç¯‡è®ºæ–‡çš„æ–¹æ³•ã€å‘ç°ã€è´¡çŒ®"""
```

#### 3.4.4 å‘é‡æ•°æ®åº“è®¾è®¡

```python
# Collectionç»“æ„
Collection: "academic_papers"

Documentç»“æ„:
{
    "item_key": "ABCD1234",
    "title": "Attention Is All You Need",
    "abstract": "å…¨æ–‡...",
    "authors": ["Vaswani, A."],
    "year": 2017,
    "tags": ["NLP", "Transformer"],
    "pdf_text": "å®Œæ•´PDFæ–‡æœ¬...",
    "zotero_created": "2024-01-01",
    "embedding": [0.1, 0.2, ...]  # text-embedding-3-small (1536ç»´)
}

# Metadata Filterç¤ºä¾‹
filters = {
    "year": {"$gte": 2020},
    "tags": {"$in": ["Deep Learning", "NLP"]},
    "authors": "Hinton"
}
```

#### 3.4.5 Promptè®¾è®¡

**PDFåˆ†æPrompt**

```python
PDF_ANALYSIS_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯ç ”ç©¶åŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œæä¾›ç»“æ„åŒ–çš„å­¦æœ¯åˆ†æã€‚

è®ºæ–‡æ ‡é¢˜ï¼š{title}
ä½œè€…ï¼š{authors}
æ‘˜è¦ï¼š{abstract}
å…¨æ–‡ï¼š{content}

è¯·æä¾›ï¼š
1. **ç ”ç©¶é—®é¢˜**ï¼ˆ1-2å¥è¯ï¼‰
2. **æ ¸å¿ƒè´¡çŒ®**ï¼ˆ3-5ç‚¹bulletï¼‰
3. **æ–¹æ³•è®º**ï¼ˆè¯¦ç»†è¯´æ˜æ–¹æ³•è®¾è®¡ï¼‰
4. **ä¸»è¦å‘ç°**ï¼ˆå…³é”®å®éªŒç»“æœï¼‰
5. **å±€é™æ€§ä¸æ‰¹è¯„**ï¼ˆè‡³å°‘2ç‚¹ï¼‰
6. **ç›¸å…³ç ”ç©¶æ–¹å‘**ï¼ˆå»ºè®®çš„æ ‡ç­¾å’Œç›¸å…³ä¸»é¢˜ï¼‰

ä»¥JSONæ ¼å¼è¿”å›ã€‚
"""
```

**å…³é”®å­—ç”ŸæˆPrompt**

```python
KEYWORD_GENERATION_PROMPT = """
ç”¨æˆ·çš„ç ”ç©¶ä¸»é¢˜ï¼š{topic}
ä¸Šä¸‹æ–‡ï¼š{context}

è¯·ç”Ÿæˆï¼š
1. 10-15ä¸ªæ ¸å¿ƒæœç´¢å…³é”®è¯ï¼ˆè‹±æ–‡å’Œä¸­æ–‡ï¼‰
2. 5-10ä¸ªç›¸å…³æ¦‚å¿µ/æœ¯è¯­
3. 5ä¸ªä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
4. 3-5ä¸ªç›¸å…³çš„å­¦æœ¯æ•°æ®åº“/æœŸåˆŠå»ºè®®

è¿”å›JSONæ ¼å¼ã€‚
"""
```

#### 3.4.6 é…ç½®æ–‡ä»¶

```json
{
  "ai": {
    "openai_api_key": "${OPENAI_API_KEY}",
    "anthropic_api_key": "${ANTHROPIC_API_KEY}",
    "default_model": "gpt-4o",
    "embedding": {
      "model": "text-embedding-3-small",
      "dimensions": 1536
    },
    "vector_db": {
      "type": "chromadb",
      "persist_directory": "~/.academic-agent/vectordb",
      "collection_name": "academic_papers"
    },
    "analysis": {
      "max_pdf_tokens": 128000,
      "chunk_size": 50000,
      "overlap": 1000
    }
  }
}
```

---

## 4. æ•°æ®æµä¸æ ¸å¿ƒå·¥ä½œæµ

### 4.1 æ•°æ®æµå‘å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RSSæº     â”‚  arXiv, Nature, Scienceç­‰
â”‚  Gmail      â”‚  è®ºæ–‡é€šçŸ¥é‚®ä»¶
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ fetch_feeds()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ paper_feeder_mcp â”‚
â”‚ - åŸå§‹æ•°æ®è·å–   â”‚
â”‚ - æ•°æ®æ¸…æ´—       â”‚
â”‚ - å»é‡æ£€æŸ¥       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ filtered_papers
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ai_mcp     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”¤ zotero_mcp  â”‚
â”‚ - å…³é”®è¯ç”Ÿæˆâ”‚  ç”¨æˆ·  â”‚ - ç”¨æˆ·åº“     â”‚
â”‚ - è¯­ä¹‰åŒ¹é…  â”‚ å…´è¶£  â”‚ - å†å²æ•°æ®   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ matched_papers
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ zotero_mcp       â”‚
â”‚ - å¯¼å…¥æ–‡çŒ®       â”‚
â”‚ - è‡ªåŠ¨æ ‡ç­¾       â”‚
â”‚ - PDFé™„ä»¶        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ sync event
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ logseq_mcp       â”‚
â”‚ - åˆ›å»ºé¡µé¢       â”‚
â”‚ - æå–å…ƒæ•°æ®     â”‚
â”‚ - å»ºç«‹é“¾æ¥       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ ¸å¿ƒå·¥ä½œæµ

#### 4.2.1 å·¥ä½œæµ1ï¼šæ¯æ—¥è®ºæ–‡è‡ªåŠ¨å‘ç°

```yaml
è§¦å‘: Cron (æ¯å¤©æ—©ä¸Š8ç‚¹)

æ­¥éª¤:
  1. paper_feeder_mcp.fetch_all_feeds()
     - è·å–æ‰€æœ‰RSSæºçš„æœ€æ–°æ¡ç›®
     - ä»Gmailè·å–è®ºæ–‡é€šçŸ¥é‚®ä»¶

  2. paper_feeder_mcp.normalize_and_dedupe()
     - æ ‡å‡†åŒ–å…ƒæ•°æ®
     - ä¸Zoteroåº“å¯¹æ¯”å»é‡

  3. ai_mcp.generate_search_keywords()
     - åŸºäºç”¨æˆ·Zoteroåº“ç”Ÿæˆå…´è¶£å…³é”®è¯

  4. ai_mcp.semantic_match()
     - å°†æ–°è®ºæ–‡ä¸ç”¨æˆ·ç ”ç©¶å…´è¶£åŒ¹é…
     - è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°

  5. zotero_mcp.batch_import()
     - å¯¼å…¥ç›¸ä¼¼åº¦>é˜ˆå€¼çš„è®ºæ–‡
     - è‡ªåŠ¨åº”ç”¨AIç”Ÿæˆçš„æ ‡ç­¾

  6. logseq_mcp.sync_from_zotero()
     - ä¸ºæ–°å¯¼å…¥è®ºæ–‡åˆ›å»ºç¬”è®°é¡µé¢
     - é¢„å¡«å……æ¨¡æ¿ç»“æ„

  7. é€šçŸ¥ç”¨æˆ·
     - å‘é€æ‘˜è¦ï¼šå¯¼å…¥Nç¯‡è®ºæ–‡
     - åˆ—å‡ºtop 5æ¨è
```

#### 4.2.2 å·¥ä½œæµ2ï¼šæ–‡çŒ®æ·±åº¦åˆ†æ

```yaml
è§¦å‘: ç”¨æˆ·è¯·æ±‚"åˆ†æè¿™ç¯‡è®ºæ–‡"

æ­¥éª¤:
  1. zotero_mcp.get_item_fulltext(item_key)
     - è·å–PDFå…¨æ–‡

  2. ai_mcp.analyze_pdf(
       item_key,
       analysis_type="comprehensive"
     )
     - æå–ç ”ç©¶é—®é¢˜ã€æ–¹æ³•ã€å‘ç°
     - æ‰¹åˆ¤æ€§åˆ†æå±€é™æ€§
     - æ¨èç›¸å…³å·¥ä½œå’Œæ ‡ç­¾

  3. logseq_mcp.update_page_block()
     - å°†AIåˆ†æç»“æœå†™å…¥Logseqç¬”è®°
     - åœ¨"AIåˆ†æ"sectionæ·»åŠ å†…å®¹

  4. ai_mcp.find_similar_papers()
     - åŸºäºåˆ†æç»“æœæœç´¢åº“ä¸­ç›¸å…³è®ºæ–‡

  5. logseq_mcp.add_linked_pages()
     - åœ¨ç¬”è®°ä¸­æ·»åŠ ç›¸å…³è®ºæ–‡é“¾æ¥
     - æ„å»ºçŸ¥è¯†ç½‘ç»œ
```

#### 4.2.3 å·¥ä½œæµ3ï¼šä¸»é¢˜ç»¼è¿°ç”Ÿæˆ

```yaml
è§¦å‘: ç”¨æˆ·è¯·æ±‚"ç”ŸæˆXä¸»é¢˜çš„ç»¼è¿°"

æ­¥éª¤:
  1. ai_mcp.generate_search_keywords(topic)
     - ç”Ÿæˆä¸»é¢˜ç›¸å…³å…³é”®è¯

  2. zotero_mcp.semantic_search(query=topic)
     - è¯­ä¹‰æœç´¢ç›¸å…³æ–‡çŒ®

  3. zotero_mcp.get_items_fulltext(item_keys)
     - æ‰¹é‡è·å–å…¨æ–‡

  4. ai_mcp.analyze_papers(papers)
     - å¹¶è¡Œåˆ†æå¤šç¯‡è®ºæ–‡

  5. ai_mcp.synthesize_literature_review()
     - ç»¼åˆåˆ†æç»“æœ
     - ç”Ÿæˆç»¼è¿°ç»“æ„ï¼š
       - å¼•è¨€
       - ä¸»è¦æ–¹æ³•åˆ†ç±»
       - å…³é”®å‘ç°æ€»ç»“
       - ç ”ç©¶gap
       - æœªæ¥æ–¹å‘

  6. logseq_mcp.create_review_page()
     - åˆ›å»ºç»¼è¿°é¡µé¢
     - å¼•ç”¨æ‰€æœ‰ç›¸å…³è®ºæ–‡
     - ç”Ÿæˆå¯è§†åŒ–å›¾è°±
```

### 4.3 çŠ¶æ€ç®¡ç†

**Zoteroä½œä¸ºçŠ¶æ€æºï¼š**
- å·²å¯¼å…¥è®ºæ–‡çš„record ID
- æ¯ç¯‡è®ºæ–‡çš„å¤„ç†çŠ¶æ€ï¼ˆæ–°å‘ç°ã€å·²åˆ†æã€å·²è¯»ï¼‰
- æ ‡ç­¾å’Œç¬”è®°çš„æ—¶é—´æˆ³

**åŒæ­¥çŠ¶æ€è¿½è¸ªï¼š**

```json
{
  "last_sync": "2025-01-15T10:30:00Z",
  "last_rss_fetch": "2025-01-15T08:00:00Z",
  "zotero_last_update": "2025-01-15T09:15:00Z",
  "logseq_last_sync": "2025-01-15T10:30:00Z",
  "vector_db_version": 3
}
```

---

## 5. é”™è¯¯å¤„ç†ä¸è¾¹ç•Œæƒ…å†µ

### 5.1 MCPæœåŠ¡å™¨çº§åˆ«çš„é”™è¯¯å¤„ç†

#### paper_feeder_mcp

```python
# RSSè·å–å¤±è´¥
try:
    feed_data = fetch_feed(url)
except FeedNotFoundError:
    return {
        "status": "error",
        "error_type": "feed_not_found",
        "message": f"RSSæº {url} æ— æ³•è®¿é—®",
        "suggestion": "è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¨åé‡è¯•"
    }

# Gmail APIé™æµ
if rate_limit_exceeded:
    return {
        "status": "warning",
        "message": "Gmail APIé™æµï¼Œéƒ¨åˆ†é‚®ä»¶æœªè·å–",
        "retry_after": 3600,
        "partial_results": fetched_emails
    }
```

#### zotero_mcp

```python
# Zoteroæœªè¿è¡Œ
if not zotero_running():
    return {
        "status": "error",
        "error_type": "zotero_not_running",
        "message": "Zoteroæœªè¿è¡Œæˆ–æœ¬åœ°APIæœªå¯ç”¨",
        "solution": "è¯·åœ¨Zoteroåå¥½è®¾ç½®ä¸­å¯ç”¨HTTPæœåŠ¡å™¨"
    }

# æ–‡çŒ®ä¸å­˜åœ¨
if item_not_found:
    return {
        "status": "error",
        "error_type": "item_not_found",
        "item_key": item_key,
        "suggestion": "è¯¥æ–‡çŒ®å¯èƒ½å·²è¢«åˆ é™¤"
    }
```

#### logseq_mcp

```python
# Logseq APIæœªå¯åŠ¨
if not logseq_api_responsive():
    return {
        "status": "error",
        "error_type": "logseq_not_running",
        "message": "Logseqæœªè¿è¡Œæˆ–æœ¬åœ°APIæœªå¯ç”¨",
        "solution": "è¯·å¯åŠ¨Logseqå¹¶åœ¨è®¾ç½®ä¸­å¯ç”¨API"
    }

# é¡µé¢å·²å­˜åœ¨
if page_exists(page_title):
    return {
        "status": "conflict",
        "error_type": "page_already_exists",
        "page_id": existing_page_id,
        "options": ["update_existing", "create_new_with_suffix", "skip"]
    }
```

#### ai_mcp

```python
# APIé™æµ/é…é¢ç”¨å°½
if openai_rate_limited():
    return {
        "status": "error",
        "error_type": "api_rate_limit",
        "message": "OpenAI APIé…é¢å·²ç”¨å°½æˆ–è§¦å‘é™æµ",
        "retry_after": get_retry_after_time(),
        "suggestion": "è¯·æ£€æŸ¥APIé…é¢æˆ–ç¨åé‡è¯•"
    }

# Tokenè¶…é™
if exceeds_token_limit(content):
    return {
        "status": "partial",
        "message": "æ–‡æ¡£è¿‡é•¿ï¼Œå·²åˆ†æ®µå¤„ç†",
        "chunks_processed": f"{current}/{total}",
        "warning": "ç»“æœå¯èƒ½ä¸å®Œæ•´ï¼Œå»ºè®®æ‰‹åŠ¨åˆå¹¶"
    }
```

### 5.2 Agentç¼–æ’çº§åˆ«çš„é”™è¯¯å¤„ç†

```python
def workflow_import_papers(topic):
    try:
        # Step 1: è·å–RSS
        feeds = paper_feeder.fetch_all()
        if feeds["status"] == "error":
            # é™çº§ï¼šä½¿ç”¨Gmailä½œä¸ºå¤‡é€‰
            feeds = paper_feeder.fetch_from_gmail()

        # Step 2: AIè¿‡æ»¤
        filtered = ai.semantic_match(feeds)
        if filtered["status"] == "error":
            # é™çº§ï¼šä½¿ç”¨å…³é”®è¯åŒ¹é…
            filtered = paper_feeder.keyword_filter(feeds, topic)

        # Step 3: å¯¼å…¥Zotero
        imported = zotero.batch_import(filtered)
        if not imported:
            logger.error("Zoteroå¯¼å…¥å¤±è´¥")
            return {"status": "partial", "papers_imported": 0}

        # Step 4: åŒæ­¥Logseq
        synced = logseq.sync_from_zotero(imported)
        if synced["status"] == "error":
            # éå…³é”®å¤±è´¥ï¼šä¸å½±å“ä¸»æµç¨‹
            logger.warning(f"LogseqåŒæ­¥å¤±è´¥: {synced['message']}")

        return {
            "status": "success",
            "papers_imported": len(imported),
            "logseq_synced": synced.get("status") == "success"
        }

    except Exception as e:
        return {
            "status": "critical",
            "error": str(e),
            "recovery_suggestion": "è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®å’Œæ—¥å¿—"
        }
```

### 5.3 æ•°æ®ä¸€è‡´æ€§ä¿è¯

```python
def resolve_sync_conflict(zotero_item, logseq_page):
    zotero_ts = zotero_item["data"]["dateModified"]
    logseq_ts = logseq_page["updated_at"]

    if zotero_ts > logseq_ts:
        # Zoteroæ›´æ–°ï¼Œè¦†ç›–Logseq
        return sync_zotero_to_logseq(zotero_item)
    elif logseq_ts > zotero_ts:
        # Logseqæ›´æ–°ï¼Œè¯¢é—®ç”¨æˆ·æˆ–ä¿ç•™å‰¯æœ¬
        return create_conflict_copy(logseq_page)
    else:
        # åŒæ—¶æ›´æ–°ï¼Œæ™ºèƒ½åˆå¹¶
        return merge_changes(zotero_item, logseq_page)
```

---

## 6. æµ‹è¯•ç­–ç•¥

### 6.1 å•å…ƒæµ‹è¯•

```python
# paper_feeder_mcpæµ‹è¯•
def test_feed_parsing():
    """æµ‹è¯•RSSè§£æ"""
    mock_rss = load_fixture("arxiv_cs_ai.xml")
    result = paper_feeder.fetch_feed("mock_url")
    assert len(result["items"]) > 0
    assert "title" in result["items"][0]

def test_deduplication():
    """æµ‹è¯•å»é‡é€»è¾‘"""
    existing_zotero = [{"title": "Paper A", "doi": "10.123/xyz"}]
    new_papers = [
        {"title": "Paper A", "doi": "10.123/xyz"},
        {"title": "Paper B"}
    ]
    result = paper_feeder.dedupe(new_papers, existing_zotero)
    assert len(result) == 1

# ai_mcpæµ‹è¯•
def test_keyword_generation():
    """æµ‹è¯•å…³é”®è¯ç”Ÿæˆ"""
    result = ai.generate_keywords(topic="transformer")
    assert "primary_keywords" in result
    assert len(result["primary_keywords"]) >= 5
```

### 6.2 é›†æˆæµ‹è¯•

```python
def test_daily_paper_discovery_workflow():
    """æµ‹è¯•å®Œæ•´è®ºæ–‡å‘ç°æµç¨‹"""
    # Setup
    mock_rss_server.add_paper("AI paper 1")
    zotero_test_library.clear()

    # Execute
    agent.handle("å¸®æˆ‘å¯¼å…¥ä»Šå¤©å…³äºAIçš„æ–°è®ºæ–‡")

    # Assert
    imported = zotero_test_library.get_items()
    assert len(imported) > 0
    logseq_pages = logseq_test_graph.get_pages()
    assert any(p["properties"]["zotero_key"] in [i["key"] for i in imported]
               for p in logseq_pages)
```

### 6.3 æ€§èƒ½æµ‹è¯•

```python
def test_large_library_semantic_search():
    """æµ‹è¯•å¤§è§„æ¨¡è¯­ä¹‰æœç´¢æ€§èƒ½"""
    create_test_library(size=10000)
    build_semantic_index()

    start_time = time.time()
    result = ai.semantic_search("deep learning", limit=20)
    duration = time.time() - start_time

    assert duration < 2.0
    assert len(result["results"]) == 20
```

---

## 7. éƒ¨ç½²ä¸é…ç½®

### 7.1 é¡¹ç›®ç»“æ„

```
academic-agent/
â”œâ”€â”€ mcp-servers/
â”‚   â”œâ”€â”€ paper_feeder_mcp/
â”‚   â”œâ”€â”€ zotero_mcp/
â”‚   â”œâ”€â”€ logseq_mcp/
â”‚   â”œâ”€â”€ ai_mcp/
â”‚   â””â”€â”€ master_agent/
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ models/
â”œâ”€â”€ docs/
â”œâ”€â”€ tests/
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

### 7.2 ç¯å¢ƒé…ç½®

**.env.example:**

```bash
# Zoteroé…ç½®
ZOTERO_LOCAL=true
ZOTERO_API_KEY=your_zotero_api_key
ZOTERO_LIBRARY_ID=your_library_id

# Logseqé…ç½®
LOGSEQ_GRAPH_PATH=~/Documents/Logseq/MyGraph
LOGSEQ_API_ENDPOINT=http://localhost:12315

# AIæœåŠ¡é…ç½®
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
DEFAULT_EMBEDDING_MODEL=text-embedding-3-small
DEFAULT_CHAT_MODEL=gpt-4o

# å‘é‡æ•°æ®åº“
VECTOR_DB_PATH=~/.academic-agent/vectordb

# Gmailé…ç½®ï¼ˆå¯é€‰ï¼‰
GMAIL_ENABLED=false
GMAIL_LABELS=Arxiv,ScienceDirect

# RSSé…ç½®
RSS_FEEDS_CONFIG_PATH=~/.academic-agent/feeds.json
RSS_FETCH_INTERVAL_HOURS=6
```

### 7.3 å®‰è£…æ–¹å¼

**æœ¬åœ°å¼€å‘å®‰è£…:**

```bash
git clone https://github.com/your-org/academic-agent.git
cd academic-agent

pip install -e ./mcp-servers/*  # å®‰è£…æ‰€æœ‰MCPæœåŠ¡å™¨
cp .env.example ~/.env
academic-agent init --interactive
academic-agent start
```

**Dockeréƒ¨ç½²:**

```yaml
# docker-compose.yml
services:
  master-agent:
    build: .
    env_file: .env
    depends_on:
      - chromadb
    ports:
      - "8080:8080"

  chromadb:
    image: chromadb/chroma:latest
    volumes:
      - ${VECTOR_DB_PATH}:/chroma/chroma
```

**Claude Desktopé…ç½®:**

```json
{
  "mcpServers": {
    "academic-agent": {
      "command": "academic-agent",
      "env": {
        "ZOTERO_LOCAL": "true",
        "OPENAI_API_KEY": "sk-..."
      }
    }
  }
}
```

---

## 8. å¼€å‘è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒåŠŸèƒ½ï¼ˆMVPï¼‰- 4å‘¨

**Week 1-2: åŸºç¡€MCPæœåŠ¡å™¨**
- [ ] paper_feeder_mcp: RSSè·å–ã€Gmailè§£æã€æ•°æ®æ¸…æ´—
- [ ] zotero_mcp: åŸºäºç°æœ‰é¡¹ç›®æ‰©å±•
- [ ] åŸºç¡€æµ‹è¯•è¦†ç›–

**Week 3: AIåŠŸèƒ½**
- [ ] ai_mcp: è¯­ä¹‰æœç´¢
- [ ] ai_mcp: å…³é”®å­—ç”Ÿæˆ
- [ ] å‘é‡ç´¢å¼•æ„å»ºå·¥å…·

**Week 4: é›†æˆä¸æµ‹è¯•**
- [ ] logseq_mcp: åŸºç¡€CRUDå’ŒåŒæ­¥
- [ ] Master Agent: ç®€å•å·¥å…·ç¼–æ’
- [ ] ç«¯åˆ°ç«¯å·¥ä½œæµ

### Phase 2: æ™ºèƒ½åˆ†æ - 3å‘¨

**Week 5-6: PDFåˆ†æ**
- [ ] ai_mcp: PDFæ·±åº¦åˆ†æå·¥å…·
- [ ] æ‰¹é‡åˆ†æå’Œå¹¶è¡Œå¤„ç†

**Week 7: é«˜çº§å·¥ä½œæµ**
- [ ] Master Agent: å¤šæ­¥éª¤å·¥ä½œæµç¼–æ’
- [ ] ä¸»é¢˜ç»¼è¿°ç”Ÿæˆ

### Phase 3: å¢å¼ºåŠŸèƒ½ - 3å‘¨

**Week 8-9: RAGèƒ½åŠ›**
- [ ] ai_mcp: åŸºäºç¬”è®°çš„RAGæ£€ç´¢
- [ ] æ™ºèƒ½æ¨èç³»ç»Ÿ

**Week 10: ç”¨æˆ·ä½“éªŒ**
- [ ] CLIå·¥å…·å®Œå–„
- [ ] é…ç½®å‘å¯¼

### Phase 4: ç”Ÿäº§åŒ– - 2å‘¨

**Week 11-12: ç¨³å®šæ€§ä¸éƒ¨ç½²**
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] å®Œæ•´æµ‹è¯•å¥—ä»¶
- [ ] Dockeréƒ¨ç½²
- [ ] æ–‡æ¡£å®Œå–„

---

## 9. å…³é”®æŠ€æœ¯ä¸å®ç°è¦ç‚¹

### 9.1 è¯­ä¹‰æœç´¢å®ç°

**ChromaDBé›†æˆ:**

```python
import chromadb

client = chromadb.PersistentClient(path="~/.academic-agent/vectordb")
collection = client.get_or_create_collection("academic_papers")

# æ·»åŠ æ–‡æ¡£
collection.add(
    documents=[paper["abstract"]],
    embeddings=[embeddings],
    metadatas=[{
        "item_key": paper["key"],
        "title": paper["title"],
        "year": paper["year"]
    }],
    ids=[paper["key"]]
)

# æŸ¥è¯¢
results = collection.query(
    query_texts=[user_query],
    n_results=10,
    where={"year": {"$gte": 2020}}
)
```

### 9.2 PDFåˆ†æå®ç°

**åˆ†å—ç­–ç•¥:**

```python
def chunk_pdf(pdf_text, max_tokens=100000):
    """å°†é•¿PDFåˆ†å—"""
    sections = split_by_sections(pdf_text)
    chunks = []

    for section in sections:
        if section["token_count"] > max_tokens:
            subchunks = split_by_paragraphs(section, max_tokens)
            chunks.extend(subchunks)
        else:
            chunks.append(section)

    return chunks

# å¹¶è¡Œåˆ†æ
results = parallel_map(analyze_chunk, chunks, max_workers=4)
final_analysis = merge_analysis_results(results)
```

### 9.3 Zotero-LogseqåŒå‘åŒæ­¥

```python
class ZoteroLogseqSync:
    def sync(self, mode="incremental"):
        if mode == "incremental":
            zotero_changes = self.get_zotero_changes(
                since=self.last_sync_timestamp
            )
            for item in zotero_changes:
                self.sync_item_to_logseq(item)

    def sync_item_to_logseq(self, item):
        page = self.logseq.find_page_by_zotero_key(item["key"])

        if page:
            self.logseq.update_page_properties(
                page["id"],
                self.extract_properties(item)
            )
        else:
            self.logseq.create_page_from_template(
                item,
                template="academic_paper"
            )
```

### 9.4 Master Agentå·¥å…·ç¼–æ’

**åŸºäºLangGraph:**

```python
from langgraph.graph import StateGraph

def build_master_agent():
    workflow = StateGraph(AgentState)

    workflow.add_node("understand_intent", understand_intent_node)
    workflow.add_node("plan_workflow", plan_workflow_node)
    workflow.add_node("execute_tools", execute_tools_node)

    workflow.set_entry_point("understand_intent")
    workflow.add_edge("understand_intent", "plan_workflow")
    workflow.add_edge("plan_workflow", "execute_tools")
    workflow.set_finish_point("execute_tools")

    return workflow.compile()
```

---

## 10. æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶

### 10.1 APIè°ƒç”¨ä¼˜åŒ–

**æ‰¹é‡å¤„ç†:**

```python
def batch_embed_texts(texts, batch_size=100):
    """æ‰¹é‡embedé™ä½æˆæœ¬"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = openai.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        embeddings.extend([e.embedding for e in response.data])
    return embeddings
```

**ç¼“å­˜æœºåˆ¶:**

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_embedding_cached(text_hash):
    return openai.embeddings.create(...)
```

### 10.2 æˆæœ¬ä¼°ç®—

**æ¯æœˆæˆæœ¬ï¼ˆå‡è®¾ï¼‰ï¼š**
- è¯­ä¹‰æœç´¢ç´¢å¼•1000ç¯‡ï¼š$0.02
- æ¯æ—¥æŸ¥è¯¢100æ¬¡ï¼š$0.06
- PDFåˆ†æ10ç¯‡ï¼š$1.00
- å…³é”®å­—ç”Ÿæˆ30æ¬¡ï¼š$0.03

**æ€»è®¡ï¼šçº¦ $1.11/æœˆ**

### 10.3 æ€§èƒ½ä¼˜åŒ–

```python
import asyncio
from asyncio import Semaphore

semaphore = Semaphore(5)  # é™æµ

async def rate_limited_api_call(func, *args):
    async with semaphore:
        return await func(*args)

# æ‰¹é‡æœç´¢
@mcp.tool
async def batch_search_items(queries: list[str]) -> str:
    """å¹¶å‘æœç´¢å¤šä¸ªæŸ¥è¯¢"""
    tasks = [search_item(q) for q in queries]
    results = await asyncio.gather(*tasks)
    return format_results(results)
```

---

## æ€»ç»“

è¿™ä¸ªå­¦æœ¯Agentç³»ç»Ÿè®¾è®¡åŒ…å«ï¼š

**æ ¸å¿ƒç‰¹æ€§:**
- å•ä¸€Master Agentç»Ÿä¸€ç¼–æ’
- å››ä¸ªä¸“é—¨MCPæœåŠ¡å™¨
- Zoteroä¸ºæ•°æ®çœŸç›¸æº
- äº‘AIé©±åŠ¨ç¡®ä¿æ•ˆæœ

**å…³é”®åŠŸèƒ½:**
- è‡ªåŠ¨åŒ–è®ºæ–‡å‘ç°ä¸å¯¼å…¥
- è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½åŒ¹é…
- PDFæ·±åº¦åˆ†æ
- Zotero-LogseqåŒå‘åŒæ­¥
- ä¸»é¢˜ç»¼è¿°ç”Ÿæˆ

**æŠ€æœ¯é€‰å‹:**
- MCPåè®®æ ‡å‡†åŒ–
- OpenAI/Anthropic API
- ChromaDBå‘é‡æ•°æ®åº“
- LangGraph Agentç¼–æ’
- å¤ç”¨zotero-mcp

**å®æ–½è®¡åˆ’:**
- Phase 1 (4å‘¨): MVP
- Phase 2 (3å‘¨): PDFåˆ†æ
- Phase 3 (3å‘¨): RAGåŠŸèƒ½
- Phase 4 (2å‘¨): ç”Ÿäº§åŒ–

**æˆæœ¬:**
- æœˆå‡çº¦$1-2
- æ€§èƒ½ä¼˜åŒ–é™ä½APIè°ƒç”¨

---

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨:**
1. å®¡é˜…å¹¶æ‰¹å‡†è®¾è®¡æ–‡æ¡£
2. åˆ›å»ºè¯¦ç»†çš„å®ç°è®¡åˆ’
3. æ­å»ºå¼€å‘ç¯å¢ƒå’Œé¡¹ç›®ç»“æ„
4. å¼€å§‹Phase 1å¼€å‘
